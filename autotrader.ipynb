{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:26:26.516557Z",
     "iopub.status.busy": "2025-07-01T04:26:26.515764Z",
     "iopub.status.idle": "2025-07-01T04:26:36.570176Z",
     "shell.execute_reply": "2025-07-01T04:26:36.569401Z",
     "shell.execute_reply.started": "2025-07-01T04:26:26.516532Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas-ta in /usr/local/lib/python3.11/dist-packages (0.3.14b0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from pandas-ta) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas-ta) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas-ta) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas-ta) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas-ta) (2025.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->pandas-ta) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->pandas-ta) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->pandas-ta) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->pandas-ta) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->pandas-ta) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->pandas-ta) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->pandas-ta) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas->pandas-ta) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas->pandas-ta) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas->pandas-ta) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas->pandas-ta) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas->pandas-ta) (2024.2.0)\n",
      "Requirement already satisfied: alpaca-trade-api in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
      "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api) (2.32.3)\n",
      "Requirement already satisfied: urllib3<2,>1.24 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api) (1.26.20)\n",
      "Requirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api) (1.8.0)\n",
      "Collecting websockets<11,>=9.0 (from alpaca-trade-api)\n",
      "  Using cached websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: msgpack==1.0.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api) (1.0.3)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api) (3.11.18)\n",
      "Requirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api) (6.0.1)\n",
      "Requirement already satisfied: deprecation==2.1.0 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api) (2.1.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from deprecation==2.1.0->alpaca-trade-api) (25.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.20.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11.1->alpaca-trade-api) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11.1->alpaca-trade-api) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11.1->alpaca-trade-api) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11.1->alpaca-trade-api) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11.1->alpaca-trade-api) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11.1->alpaca-trade-api) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>2->alpaca-trade-api) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>2->alpaca-trade-api) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>2->alpaca-trade-api) (2025.4.26)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.18.1->alpaca-trade-api) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.11.1->alpaca-trade-api) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.11.1->alpaca-trade-api) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.11.1->alpaca-trade-api) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.11.1->alpaca-trade-api) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.11.1->alpaca-trade-api) (2024.2.0)\n",
      "Using cached websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (107 kB)\n",
      "Installing collected packages: websockets\n",
      "  Attempting uninstall: websockets\n",
      "    Found existing installation: websockets 15.0.1\n",
      "    Uninstalling websockets-15.0.1:\n",
      "      Successfully uninstalled websockets-15.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "yfinance 0.2.64 requires websockets>=13.0, but you have websockets 10.4 which is incompatible.\n",
      "google-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-genai 1.9.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed websockets-10.4\n",
      "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.64)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.8)\n",
      "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.3)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.11.4)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.20.3)\n",
      "Collecting websockets>=13.0 (from yfinance)\n",
      "  Using cached websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.13.2)\n",
      "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (2025.4.26)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.5->yfinance) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.5->yfinance) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.5->yfinance) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.5->yfinance) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.5->yfinance) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.5->yfinance) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (1.26.20)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.5->yfinance) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.5->yfinance) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.5->yfinance) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.16.5->yfinance) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.16.5->yfinance) (2024.2.0)\n",
      "Using cached websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "Installing collected packages: websockets\n",
      "  Attempting uninstall: websockets\n",
      "    Found existing installation: websockets 10.4\n",
      "    Uninstalling websockets-10.4:\n",
      "      Successfully uninstalled websockets-10.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "alpaca-trade-api 3.2.0 requires websockets<11,>=9.0, but you have websockets 15.0.1 which is incompatible.\n",
      "google-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed websockets-15.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas-ta\n",
    "!pip install alpaca-trade-api\n",
    "!pip install --upgrade yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-01T04:26:36.572074Z",
     "iopub.status.busy": "2025-07-01T04:26:36.571861Z",
     "iopub.status.idle": "2025-07-01T04:26:36.589997Z",
     "shell.execute_reply": "2025-07-01T04:26:36.589431Z",
     "shell.execute_reply.started": "2025-07-01T04:26:36.572054Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoGluon not available. Install with: pip install autogluon\n",
      "TabPFN not available. Install with: pip install tabpfn\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Data collection\n",
    "import yfinance as yf\n",
    "import pandas_ta as ta\n",
    "\n",
    "# Machine Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Gradient Boosting\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# AutoML\n",
    "try:\n",
    "    from autogluon.tabular import TabularPredictor\n",
    "    AUTOGLUON_AVAILABLE = True\n",
    "except ImportError:\n",
    "    AUTOGLUON_AVAILABLE = False\n",
    "    print(\"AutoGluon not available. Install with: pip install autogluon\")\n",
    "\n",
    "# TabPFN (if available)\n",
    "try:\n",
    "    from tabpfn import TabPFNRegressor\n",
    "    TABPFN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TABPFN_AVAILABLE = False\n",
    "    print(\"TabPFN not available. Install with: pip install tabpfn\")\n",
    "\n",
    "# Trading\n",
    "import alpaca_trade_api as tradeapi\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:26:36.590952Z",
     "iopub.status.busy": "2025-07-01T04:26:36.590737Z",
     "iopub.status.idle": "2025-07-01T04:26:36.606678Z",
     "shell.execute_reply": "2025-07-01T04:26:36.606112Z",
     "shell.execute_reply.started": "2025-07-01T04:26:36.590928Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Available: True\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration class for the trading system\"\"\"\n",
    "    \n",
    "    # Data parameters\n",
    "    LOOKBACK_DAYS: int = 252 * 2  # 2 years of data\n",
    "    PREDICTION_HORIZON: int = 5   # 5 days ahead prediction\n",
    "    FEATURE_WINDOW: int = 20      # Technical indicator window\n",
    "    \n",
    "    # Model parameters\n",
    "    TRAIN_TEST_SPLIT: float = 0.8\n",
    "    VALIDATION_SPLIT: float = 0.2\n",
    "    CROSS_VAL_FOLDS: int = 5\n",
    "    \n",
    "    # Portfolio parameters\n",
    "    PORTFOLIO_ALLOCATION: float = 0.9  # 90% of portfolio\n",
    "    MAX_POSITION_SIZE: float = 0.1     # Max 10% per stock\n",
    "    MIN_CONFIDENCE: float = 0.6        # Minimum prediction confidence\n",
    "    \n",
    "    # Risk parameters\n",
    "    MAX_DRAWDOWN: float = 0.15         # 15% max drawdown\n",
    "    VAR_CONFIDENCE: float = 0.95       # 95% VaR\n",
    "    MONTE_CARLO_SIMS: int = 10000      # Monte Carlo simulations\n",
    "    \n",
    "    # GPU settings\n",
    "    USE_GPU: bool = torch.cuda.is_available()\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # File paths\n",
    "    MODEL_SAVE_PATH: str = \"./models/\"\n",
    "    DATA_SAVE_PATH: str = \"./data/\"\n",
    "    RESULTS_SAVE_PATH: str = \"./results/\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for path in [config.MODEL_SAVE_PATH, config.DATA_SAVE_PATH, config.RESULTS_SAVE_PATH]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "print(f\"Using device: {config.DEVICE}\")\n",
    "print(f\"GPU Available: {config.USE_GPU}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:26:36.608352Z",
     "iopub.status.busy": "2025-07-01T04:26:36.608148Z",
     "iopub.status.idle": "2025-07-01T04:26:36.637410Z",
     "shell.execute_reply": "2025-07-01T04:26:36.636732Z",
     "shell.execute_reply.started": "2025-07-01T04:26:36.608337Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DataCollector:\n",
    "    \"\"\"Handles data collection and feature engineering for stocks\"\"\"\n",
    "    \n",
    "    def __init__(self, tickers: List[str]):\n",
    "        self.tickers = tickers\n",
    "        self.data = {}\n",
    "        self.features = {}\n",
    "        \n",
    "    def fetch_stock_data(self, ticker: str, period: str = \"2y\") -> pd.DataFrame:\n",
    "        \"\"\"Fetch stock data from Yahoo Finance\"\"\"\n",
    "        try:\n",
    "            stock = yf.Ticker(ticker)\n",
    "            data = stock.history(period=period)\n",
    "            \n",
    "            if data.empty:\n",
    "                print(f\"No data found for {ticker}\")\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "            # Clean data\n",
    "            data = data.dropna()\n",
    "            data.index = pd.to_datetime(data.index)\n",
    "            \n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {ticker}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def calculate_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate comprehensive technical indicators with better feature diversity\"\"\"\n",
    "        if df.empty:\n",
    "            return df\n",
    "            \n",
    "        # Normalize prices to create stock-specific features\n",
    "        df['Price_norm'] = df['Close'] / df['Close'].iloc[0]  # Normalized to first price\n",
    "        df['Volume_norm'] = df['Volume'] / df['Volume'].mean()  # Normalized volume\n",
    "        \n",
    "        # Price-based indicators with multiple timeframes\n",
    "        for period in [5, 10, 20, 50]:\n",
    "            df[f'SMA_{period}'] = ta.sma(df['Close'], length=period)\n",
    "            df[f'EMA_{period}'] = ta.ema(df['Close'], length=period)\n",
    "            df[f'Price_vs_SMA_{period}'] = (df['Close'] - df[f'SMA_{period}']) / df[f'SMA_{period}']\n",
    "            df[f'SMA_slope_{period}'] = df[f'SMA_{period}'].pct_change(periods=5)\n",
    "        \n",
    "        # Advanced momentum indicators\n",
    "        df['RSI'] = ta.rsi(df['Close'], length=14)\n",
    "        df['RSI_14'] = ta.rsi(df['Close'], length=14)\n",
    "        df['RSI_21'] = ta.rsi(df['Close'], length=21)\n",
    "        df['RSI_divergence'] = df['RSI_14'] - df['RSI_21']\n",
    "        \n",
    "        # MACD with multiple timeframes\n",
    "        macd_12_26 = ta.macd(df['Close'], fast=12, slow=26)\n",
    "        macd_8_21 = ta.macd(df['Close'], fast=8, slow=21)\n",
    "        df['MACD'] = macd_12_26['MACD_12_26_9']\n",
    "        df['MACD_signal'] = macd_12_26['MACDs_12_26_9']\n",
    "        df['MACD_hist'] = macd_12_26['MACDh_12_26_9']\n",
    "        df['MACD_fast'] = macd_8_21['MACD_8_21_9']\n",
    "        df['MACD_crossover'] = (df['MACD'] > df['MACD_signal']).astype(int)\n",
    "        \n",
    "        # Volatility indicators\n",
    "        bb_20 = ta.bbands(df['Close'], length=20)\n",
    "        bb_10 = ta.bbands(df['Close'], length=10)\n",
    "        df['BB_upper_20'] = bb_20['BBU_20_2.0']\n",
    "        df['BB_middle_20'] = bb_20['BBM_20_2.0']\n",
    "        df['BB_lower_20'] = bb_20['BBL_20_2.0']\n",
    "        df['BB_width_20'] = (df['BB_upper_20'] - df['BB_lower_20']) / df['BB_middle_20']\n",
    "        df['BB_position_20'] = (df['Close'] - df['BB_lower_20']) / (df['BB_upper_20'] - df['BB_lower_20'])\n",
    "        \n",
    "        df['BB_upper_10'] = bb_10['BBU_10_2.0']\n",
    "        df['BB_width_10'] = (bb_10['BBU_10_2.0'] - bb_10['BBL_10_2.0']) / bb_10['BBM_10_2.0']\n",
    "        \n",
    "        # Advanced volume indicators\n",
    "        df['Volume_SMA_20'] = ta.sma(df['Volume'], length=20)\n",
    "        df['Volume_SMA_5'] = ta.sma(df['Volume'], length=5)\n",
    "        df['Volume_ratio_20'] = df['Volume'] / df['Volume_SMA_20']\n",
    "        df['Volume_ratio_5'] = df['Volume'] / df['Volume_SMA_5']\n",
    "        df['OBV'] = ta.obv(df['Close'], df['Volume'])\n",
    "        df['OBV_slope'] = df['OBV'].pct_change(periods=5)\n",
    "        \n",
    "        # Price action and momentum features\n",
    "        df['Price_change'] = df['Close'].pct_change()\n",
    "        df['Price_change_abs'] = df['Price_change'].abs()\n",
    "        df['High_Low_ratio'] = (df['High'] - df['Low']) / df['Close']\n",
    "        df['Close_Open_ratio'] = (df['Close'] - df['Open']) / df['Open']\n",
    "        df['High_Close_ratio'] = (df['High'] - df['Close']) / df['Close']\n",
    "        df['Close_Low_ratio'] = (df['Close'] - df['Low']) / df['Close']\n",
    "        \n",
    "        # Volatility measures\n",
    "        for window in [5, 10, 20]:\n",
    "            df[f'Volatility_{window}'] = df['Price_change'].rolling(window=window).std()\n",
    "        \n",
    "        # Trend strength indicators\n",
    "        df['ADX'] = ta.adx(df['High'], df['Low'], df['Close'])['ADX_14']\n",
    "        df['CCI'] = ta.cci(df['High'], df['Low'], df['Close'], length=20)\n",
    "        df['Williams_R'] = ta.willr(df['High'], df['Low'], df['Close'], length=14)\n",
    "        \n",
    "        # Price position indicators\n",
    "        for window in [10, 20, 50]:\n",
    "            df[f'Price_position_{window}'] = (df['Close'] - df['Close'].rolling(window=window).min()) / \\\n",
    "                                           (df['Close'].rolling(window=window).max() - df['Close'].rolling(window=window).min())\n",
    "            df[f'Volume_position_{window}'] = (df['Volume'] - df['Volume'].rolling(window=window).min()) / \\\n",
    "                                            (df['Volume'].rolling(window=window).max() - df['Volume'].rolling(window=window).min())\n",
    "        \n",
    "        # Lag features with different periods\n",
    "        for lag in [1, 2, 3, 5, 10]:\n",
    "            df[f'Return_lag_{lag}'] = df['Price_change'].shift(lag)\n",
    "            df[f'Volume_change_lag_{lag}'] = df['Volume'].pct_change().shift(lag)\n",
    "            df[f'RSI_lag_{lag}'] = df['RSI'].shift(lag)\n",
    "            df[f'BB_position_lag_{lag}'] = df['BB_position_20'].shift(lag)\n",
    "        \n",
    "        # Rolling statistics with multiple windows\n",
    "        for window in [5, 10, 20]:\n",
    "            df[f'Return_mean_{window}'] = df['Price_change'].rolling(window=window).mean()\n",
    "            df[f'Return_std_{window}'] = df['Price_change'].rolling(window=window).std()\n",
    "            df[f'Return_skew_{window}'] = df['Price_change'].rolling(window=window).skew()\n",
    "            df[f'Return_kurt_{window}'] = df['Price_change'].rolling(window=window).kurt()\n",
    "            df[f'Return_min_{window}'] = df['Price_change'].rolling(window=window).min()\n",
    "            df[f'Return_max_{window}'] = df['Price_change'].rolling(window=window).max()\n",
    "        \n",
    "        # Cross-asset momentum (if multiple timeframes)\n",
    "        df['Momentum_5'] = df['Close'] / df['Close'].shift(5) - 1\n",
    "        df['Momentum_10'] = df['Close'] / df['Close'].shift(10) - 1\n",
    "        df['Momentum_20'] = df['Close'] / df['Close'].shift(20) - 1\n",
    "        \n",
    "        # Rate of change indicators\n",
    "        for period in [5, 10, 20]:\n",
    "            df[f'ROC_{period}'] = ta.roc(df['Close'], length=period)\n",
    "            df[f'RSI_ROC_{period}'] = ta.roc(df['RSI'], length=period)\n",
    "        \n",
    "        # Stochastic indicators\n",
    "        stoch = ta.stoch(df['High'], df['Low'], df['Close'])\n",
    "        df['Stoch_K'] = stoch['STOCHk_14_3_3']\n",
    "        df['Stoch_D'] = stoch['STOCHd_14_3_3']\n",
    "        df['Stoch_diff'] = df['Stoch_K'] - df['Stoch_D']\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_target_variable(self, df: pd.DataFrame, horizon: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"Create target variable for prediction\"\"\"\n",
    "        if df.empty:\n",
    "            return df\n",
    "            \n",
    "        # Future return over horizon days\n",
    "        df['target_return'] = df['Close'].shift(-horizon) / df['Close'] - 1\n",
    "        \n",
    "        # Future volatility\n",
    "        df['target_volatility'] = df['Price_change'].shift(-horizon).rolling(window=horizon).std()\n",
    "        \n",
    "        # Future direction (classification target)\n",
    "        df['target_direction'] = (df['target_return'] > 0).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def collect_all_data(self) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Collect and process data for all tickers\"\"\"\n",
    "        print(\"Starting data collection...\")\n",
    "        \n",
    "        for ticker in self.tickers:\n",
    "            print(f\"Processing {ticker}...\")\n",
    "            \n",
    "            # Fetch raw data\n",
    "            raw_data = self.fetch_stock_data(ticker)\n",
    "            if raw_data.empty:\n",
    "                continue\n",
    "                \n",
    "            # Calculate technical indicators\n",
    "            processed_data = self.calculate_technical_indicators(raw_data)\n",
    "            \n",
    "            # Create target variables\n",
    "            processed_data = self.create_target_variable(processed_data, config.PREDICTION_HORIZON)\n",
    "            \n",
    "            # Store data\n",
    "            self.data[ticker] = processed_data\n",
    "            \n",
    "        print(f\"Data collection completed for {len(self.data)} tickers\")\n",
    "        return self.data\n",
    "    \n",
    "    def prepare_features(self, ticker: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Prepare features and targets for ML models\"\"\"\n",
    "        if ticker not in self.data:\n",
    "            return np.array([]), np.array([])\n",
    "            \n",
    "        df = self.data[ticker].copy()\n",
    "        \n",
    "        # Select feature columns (exclude OHLCV and target columns)\n",
    "        feature_cols = [col for col in df.columns if not col.startswith(('Open', 'High', 'Low', 'Close', 'Volume', 'target_'))]\n",
    "        \n",
    "        # Remove rows with NaN values\n",
    "        df = df.dropna()\n",
    "        \n",
    "        if df.empty:\n",
    "            return np.array([]), np.array([])\n",
    "        \n",
    "        X = df[feature_cols].values\n",
    "        y = df['target_return'].values\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:26:36.638398Z",
     "iopub.status.busy": "2025-07-01T04:26:36.638175Z",
     "iopub.status.idle": "2025-07-01T04:26:36.661827Z",
     "shell.execute_reply": "2025-07-01T04:26:36.661186Z",
     "shell.execute_reply.started": "2025-07-01T04:26:36.638369Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MLPRegressor(nn.Module):\n",
    "    \"\"\"Improved Multi-Layer Perceptron for stock prediction with better architecture\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_sizes: List[int] = [512, 256, 128, 64], \n",
    "                 dropout_rate: float = 0.2, use_batch_norm: bool = True):\n",
    "        super(MLPRegressor, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # Input layer with batch norm\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(input_size))\n",
    "        \n",
    "        for i, hidden_size in enumerate(hidden_sizes):\n",
    "            # Linear layer\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            \n",
    "            # Batch normalization\n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            \n",
    "            # Activation - use different activations for variety\n",
    "            if i % 2 == 0:\n",
    "                layers.append(nn.ReLU())\n",
    "            else:\n",
    "                layers.append(nn.LeakyReLU(0.1))\n",
    "            \n",
    "            # Dropout with different rates\n",
    "            dropout = dropout_rate * (1 + i * 0.1)  # Increasing dropout in deeper layers\n",
    "            layers.append(nn.Dropout(min(dropout, 0.5)))\n",
    "            \n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer with no activation (regression)\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights properly\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Proper weight initialization\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, nn.BatchNorm1d):\n",
    "            nn.init.constant_(module.weight, 1)\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class MLPTrainer:\n",
    "    \"\"\"Improved trainer class for MLP models with better training strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, device: str = config.DEVICE):\n",
    "        self.device = device\n",
    "        self.model = MLPRegressor(input_size).to(device)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        # Use different optimizers and learning rates\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            self.optimizer, T_0=20, T_mult=2, eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        # Early stopping\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.patience = 20\n",
    "        self.patience_counter = 0\n",
    "        \n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray, \n",
    "              X_val: np.ndarray, y_val: np.ndarray, \n",
    "              epochs: int = 200, batch_size: int = 128) -> Dict[str, List[float]]:\n",
    "        \"\"\"Train the MLP model with improved training loop\"\"\"\n",
    "        \n",
    "        # Robust scaling to prevent identical predictions\n",
    "        scaler_X = RobustScaler()\n",
    "        scaler_y = RobustScaler()\n",
    "        \n",
    "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "        X_val_scaled = scaler_X.transform(X_val)\n",
    "        y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "        y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Store scalers for later use\n",
    "        self.scaler_X = scaler_X\n",
    "        self.scaler_y = scaler_y\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_train_tensor = torch.FloatTensor(X_train_scaled).to(self.device)\n",
    "        y_train_tensor = torch.FloatTensor(y_train_scaled).reshape(-1, 1).to(self.device)\n",
    "        X_val_tensor = torch.FloatTensor(X_val_scaled).to(self.device)\n",
    "        y_val_tensor = torch.FloatTensor(y_val_scaled).reshape(-1, 1).to(self.device)\n",
    "        \n",
    "        # Create data loaders with shuffling\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        \n",
    "        history = {'train_loss': [], 'val_loss': []}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for batch_X, batch_y in train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = self.criterion(outputs, batch_y)\n",
    "                \n",
    "                # Add L1 regularization\n",
    "                l1_lambda = 1e-5\n",
    "                l1_norm = sum(p.abs().sum() for p in self.model.parameters())\n",
    "                loss = loss + l1_lambda * l1_norm\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = self.model(X_val_tensor)\n",
    "                val_loss = self.criterion(val_outputs, y_val_tensor).item()\n",
    "            \n",
    "            # Update learning rate\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Record history\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            history['train_loss'].append(avg_train_loss)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.patience_counter = 0\n",
    "                # Save best model state\n",
    "                self.best_model_state = self.model.state_dict().copy()\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                \n",
    "            if self.patience_counter >= self.patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "            \n",
    "            if epoch % 50 == 0:\n",
    "                print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.6f}, Val Loss = {val_loss:.6f}, LR = {self.scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        # Load best model\n",
    "        if hasattr(self, 'best_model_state'):\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions with proper scaling\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Scale input\n",
    "            X_scaled = self.scaler_X.transform(X)\n",
    "            X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions_scaled = self.model(X_tensor).cpu().numpy().flatten()\n",
    "            \n",
    "            # Inverse scale predictions\n",
    "            predictions = self.scaler_y.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()\n",
    "            \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:26:36.662777Z",
     "iopub.status.busy": "2025-07-01T04:26:36.662549Z",
     "iopub.status.idle": "2025-07-01T04:26:36.683599Z",
     "shell.execute_reply": "2025-07-01T04:26:36.683032Z",
     "shell.execute_reply.started": "2025-07-01T04:26:36.662737Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GradientBoostingEnsemble:\n",
    "    \"\"\"Ensemble of gradient boosting models\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        \n",
    "    def create_xgboost_model(self) -> xgb.XGBRegressor:\n",
    "        \"\"\"Create XGBoost model with GPU support if available\"\"\"\n",
    "        params = {\n",
    "            'n_estimators': 1000,\n",
    "            'max_depth': 6,\n",
    "            'learning_rate': 0.01,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1,\n",
    "            'early_stopping_rounds': 50\n",
    "        }\n",
    "        \n",
    "        if config.USE_GPU:\n",
    "            params['tree_method'] = 'gpu_hist'\n",
    "            params['gpu_id'] = 0\n",
    "            \n",
    "        return xgb.XGBRegressor(**params)\n",
    "    \n",
    "    def create_lightgbm_model(self) -> lgb.LGBMRegressor:\n",
    "        \"\"\"Create LightGBM model with GPU support if available\"\"\"\n",
    "        params = {\n",
    "            'n_estimators': 1000,\n",
    "            'max_depth': 6,\n",
    "            'learning_rate': 0.01,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        \n",
    "        if config.USE_GPU:\n",
    "            params['device'] = 'gpu'\n",
    "            \n",
    "        return lgb.LGBMRegressor(**params)\n",
    "    \n",
    "    def create_base_models(self) -> Dict[str, Any]:\n",
    "        \"\"\"Create base models for boosting\"\"\"\n",
    "        base_models = {\n",
    "            'lasso': Lasso(alpha=0.01, random_state=42),\n",
    "            'svr': SVR(kernel='rbf', C=1.0, gamma='scale'),\n",
    "            'rf': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        }\n",
    "        return base_models\n",
    "    \n",
    "    def train_boosted_models(self, X_train: np.ndarray, y_train: np.ndarray,\n",
    "                           X_val: np.ndarray, y_val: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Train gradient boosting models over base models\"\"\"\n",
    "        \n",
    "        base_models = self.create_base_models()\n",
    "        boosted_models = {}\n",
    "        \n",
    "        for name, base_model in base_models.items():\n",
    "            print(f\"Training boosted model over {name}...\")\n",
    "            \n",
    "            # Train base model\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_val_scaled = scaler.transform(X_val)\n",
    "            \n",
    "            base_model.fit(X_train_scaled, y_train)\n",
    "            base_pred_train = base_model.predict(X_train_scaled)\n",
    "            base_pred_val = base_model.predict(X_val_scaled)\n",
    "            \n",
    "            # Calculate residuals\n",
    "            residuals_train = y_train - base_pred_train\n",
    "            residuals_val = y_val - base_pred_val\n",
    "            \n",
    "            # Train XGBoost on residuals\n",
    "            xgb_model = self.create_xgboost_model()\n",
    "            xgb_model.fit(X_train, residuals_train, \n",
    "                         eval_set=[(X_val, residuals_val)], \n",
    "                         verbose=False)\n",
    "            \n",
    "            boosted_models[f'boosted_{name}'] = {\n",
    "                'base_model': base_model,\n",
    "                'boost_model': xgb_model,\n",
    "                'scaler': scaler\n",
    "            }\n",
    "        \n",
    "        return boosted_models\n",
    "    \n",
    "    def train_all_models(self, X_train: np.ndarray, y_train: np.ndarray,\n",
    "                        X_val: np.ndarray, y_val: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Train all gradient boosting models\"\"\"\n",
    "        \n",
    "        all_models = {}\n",
    "        \n",
    "        # Standard XGBoost\n",
    "        print(\"Training XGBoost...\")\n",
    "        xgb_model = self.create_xgboost_model()\n",
    "        xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        all_models['xgboost'] = xgb_model\n",
    "        \n",
    "        # Standard LightGBM\n",
    "        print(\"Training LightGBM...\")\n",
    "        lgb_model = self.create_lightgbm_model()\n",
    "        lgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                     callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\n",
    "        all_models['lightgbm'] = lgb_model\n",
    "        \n",
    "        # Boosted models\n",
    "        boosted_models = self.train_boosted_models(X_train, y_train, X_val, y_val)\n",
    "        all_models.update(boosted_models)\n",
    "        \n",
    "        return all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:26:36.684707Z",
     "iopub.status.busy": "2025-07-01T04:26:36.684409Z",
     "iopub.status.idle": "2025-07-01T04:26:36.701361Z",
     "shell.execute_reply": "2025-07-01T04:26:36.700806Z",
     "shell.execute_reply.started": "2025-07-01T04:26:36.684685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AutoMLEnsemble:\n",
    "    \"\"\"AutoML and TabPFN model ensemble\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        \n",
    "    def train_autogluon(self, X_train: np.ndarray, y_train: np.ndarray,\n",
    "                       X_val: np.ndarray, y_val: np.ndarray) -> Optional[Any]:\n",
    "        \"\"\"Train AutoGluon model if available\"\"\"\n",
    "        if not AUTOGLUON_AVAILABLE:\n",
    "            print(\"AutoGluon not available\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Prepare data for AutoGluon\n",
    "            train_df = pd.DataFrame(X_train)\n",
    "            train_df['target'] = y_train\n",
    "            \n",
    "            val_df = pd.DataFrame(X_val)\n",
    "            val_df['target'] = y_val\n",
    "            \n",
    "            # Train AutoGluon\n",
    "            predictor = TabularPredictor(\n",
    "                label='target',\n",
    "                problem_type='regression',\n",
    "                eval_metric='mean_squared_error',\n",
    "                path=os.path.join(config.MODEL_SAVE_PATH, 'autogluon_models')\n",
    "            )\n",
    "            \n",
    "            predictor.fit(\n",
    "                train_data=train_df,\n",
    "                tuning_data=val_df,\n",
    "                time_limit=600,  # 10 minutes\n",
    "                presets='best_quality'\n",
    "            )\n",
    "            \n",
    "            return predictor\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training AutoGluon: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def train_tabpfn(self, X_train: np.ndarray, y_train: np.ndarray) -> Optional[Any]:\n",
    "        \"\"\"Train TabPFN model if available\"\"\"\n",
    "        if not TABPFN_AVAILABLE:\n",
    "            print(\"TabPFN not available\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # TabPFN has limitations on dataset size\n",
    "            if X_train.shape[0] > 1000 or X_train.shape[1] > 100:\n",
    "                # Sample data if too large\n",
    "                indices = np.random.choice(X_train.shape[0], size=min(1000, X_train.shape[0]), replace=False)\n",
    "                X_train_sample = X_train[indices]\n",
    "                y_train_sample = y_train[indices]\n",
    "                \n",
    "                # Select top features if too many\n",
    "                if X_train.shape[1] > 100:\n",
    "                    from sklearn.feature_selection import SelectKBest, f_regression\n",
    "                    selector = SelectKBest(f_regression, k=100)\n",
    "                    X_train_sample = selector.fit_transform(X_train_sample, y_train_sample)\n",
    "            else:\n",
    "                X_train_sample = X_train\n",
    "                y_train_sample = y_train\n",
    "            \n",
    "            model = TabPFNRegressor(device=config.DEVICE)\n",
    "            model.fit(X_train_sample, y_train_sample)\n",
    "            \n",
    "            return model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training TabPFN: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:26:36.702430Z",
     "iopub.status.busy": "2025-07-01T04:26:36.702197Z",
     "iopub.status.idle": "2025-07-01T04:26:36.720924Z",
     "shell.execute_reply": "2025-07-01T04:26:36.720268Z",
     "shell.execute_reply.started": "2025-07-01T04:26:36.702410Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EnsembleStacker:\n",
    "    \"\"\"Multi-level ensemble stacking system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.level1_models = {}\n",
    "        self.level2_models = {}\n",
    "        self.level3_model = None\n",
    "        self.scalers = {}\n",
    "        \n",
    "    def train_level1_models(self, X_train: np.ndarray, y_train: np.ndarray,\n",
    "                           X_val: np.ndarray, y_val: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Train Level 1 base models\"\"\"\n",
    "        \n",
    "        level1_models = {}\n",
    "        \n",
    "        # Neural Network MLP\n",
    "        print(\"Training Level 1: Neural Network MLP...\")\n",
    "        mlp_trainer = MLPTrainer(X_train.shape[1])\n",
    "        mlp_trainer.train(X_train, y_train, X_val, y_val, epochs=100)\n",
    "        level1_models['nn_mlp'] = mlp_trainer\n",
    "        \n",
    "        # Gradient Boosting Models\n",
    "        print(\"Training Level 1: Gradient Boosting Models...\")\n",
    "        gb_ensemble = GradientBoostingEnsemble()\n",
    "        gb_models = gb_ensemble.train_all_models(X_train, y_train, X_val, y_val)\n",
    "        level1_models.update(gb_models)\n",
    "        \n",
    "        # AutoML Models\n",
    "        print(\"Training Level 1: AutoML Models...\")\n",
    "        automl_ensemble = AutoMLEnsemble()\n",
    "        \n",
    "        autogluon_model = automl_ensemble.train_autogluon(X_train, y_train, X_val, y_val)\n",
    "        if autogluon_model:\n",
    "            level1_models['autogluon'] = autogluon_model\n",
    "            \n",
    "        tabpfn_model = automl_ensemble.train_tabpfn(X_train, y_train)\n",
    "        if tabpfn_model:\n",
    "            level1_models['tabpfn'] = tabpfn_model\n",
    "        \n",
    "        return level1_models\n",
    "    \n",
    "    def generate_level1_predictions(self, models: Dict[str, Any], X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Generate predictions from Level 1 models\"\"\"\n",
    "        \n",
    "        predictions = []\n",
    "        model_names = []\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            try:\n",
    "                if name == 'nn_mlp':\n",
    "                    pred = model.predict(X)\n",
    "                elif name == 'autogluon':\n",
    "                    pred_df = pd.DataFrame(X)\n",
    "                    pred = model.predict(pred_df).values\n",
    "                elif name == 'tabpfn':\n",
    "                    pred = model.predict(X)\n",
    "                elif name.startswith('boosted_'):\n",
    "                    # Handle boosted models\n",
    "                    base_model = model['base_model']\n",
    "                    boost_model = model['boost_model']\n",
    "                    scaler = model['scaler']\n",
    "                    \n",
    "                    X_scaled = scaler.transform(X)\n",
    "                    base_pred = base_model.predict(X_scaled)\n",
    "                    boost_pred = boost_model.predict(X)\n",
    "                    pred = base_pred + boost_pred\n",
    "                else:\n",
    "                    # Standard sklearn-like models\n",
    "                    pred = model.predict(X)\n",
    "                \n",
    "                predictions.append(pred.reshape(-1, 1))\n",
    "                model_names.append(name)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error generating predictions for {name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if predictions:\n",
    "            return np.hstack(predictions), model_names\n",
    "        else:\n",
    "            return np.array([]), []\n",
    "    \n",
    "    def train_level2_models(self, level1_predictions: np.ndarray, y_train: np.ndarray,\n",
    "                           level1_val_predictions: np.ndarray, y_val: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Train Level 2 meta-models\"\"\"\n",
    "        \n",
    "        level2_models = {}\n",
    "        \n",
    "        # XGBoost Level 2\n",
    "        print(\"Training Level 2: XGBoost...\")\n",
    "        xgb_l2 = xgb.XGBRegressor(\n",
    "            n_estimators=500,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.05,\n",
    "            random_state=42\n",
    "        )\n",
    "        if config.USE_GPU:\n",
    "            xgb_l2.set_params(tree_method='gpu_hist', gpu_id=0)\n",
    "        \n",
    "        xgb_l2.fit(level1_predictions, y_train,\n",
    "                   eval_set=[(level1_val_predictions, y_val)],\n",
    "                   verbose=False)\n",
    "        level2_models['xgboost_l2'] = xgb_l2\n",
    "        \n",
    "        # Neural Network Level 2\n",
    "        print(\"Training Level 2: Neural Network...\")\n",
    "        nn_l2_trainer = MLPTrainer(level1_predictions.shape[1])\n",
    "        nn_l2_trainer.train(level1_predictions, y_train, \n",
    "                           level1_val_predictions, y_val, epochs=50)\n",
    "        level2_models['nn_mlp_l2'] = nn_l2_trainer\n",
    "        \n",
    "        return level2_models\n",
    "    \n",
    "    def train_level3_model(self, level2_predictions: np.ndarray, y_train: np.ndarray) -> Any:\n",
    "        \"\"\"Train Level 3 final ensemble model (Weighted Average)\"\"\"\n",
    "        \n",
    "        print(\"Training Level 3: Weighted Average...\")\n",
    "        \n",
    "        # Use Ridge regression to learn optimal weights\n",
    "        ridge_weights = Ridge(alpha=1.0, random_state=42)\n",
    "        ridge_weights.fit(level2_predictions, y_train)\n",
    "        \n",
    "        return ridge_weights\n",
    "    \n",
    "    def fit(self, X_train: np.ndarray, y_train: np.ndarray,\n",
    "            X_val: np.ndarray, y_val: np.ndarray) -> 'EnsembleStacker':\n",
    "        \"\"\"Fit the complete ensemble stacking system\"\"\"\n",
    "        \n",
    "        print(\"Starting ensemble stacking training...\")\n",
    "        \n",
    "        # Level 1: Train base models\n",
    "        self.level1_models = self.train_level1_models(X_train, y_train, X_val, y_val)\n",
    "        \n",
    "        # Generate Level 1 predictions\n",
    "        level1_train_preds, model_names = self.generate_level1_predictions(self.level1_models, X_train)\n",
    "        level1_val_preds, _ = self.generate_level1_predictions(self.level1_models, X_val)\n",
    "        \n",
    "        if level1_train_preds.size == 0:\n",
    "            raise ValueError(\"No valid Level 1 predictions generated\")\n",
    "        \n",
    "        # Level 2: Train meta-models\n",
    "        self.level2_models = self.train_level2_models(level1_train_preds, y_train,\n",
    "                                                     level1_val_preds, y_val)\n",
    "        \n",
    "        # Generate Level 2 predictions\n",
    "        level2_train_preds, _ = self.generate_level1_predictions(self.level2_models, level1_train_preds)\n",
    "        \n",
    "        # Level 3: Train final ensemble\n",
    "        if level2_train_preds.size > 0:\n",
    "            self.level3_model = self.train_level3_model(level2_train_preds, y_train)\n",
    "        \n",
    "        print(\"Ensemble stacking training completed!\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions using the complete ensemble\"\"\"\n",
    "        \n",
    "        # Level 1 predictions\n",
    "        level1_preds, _ = self.generate_level1_predictions(self.level1_models, X)\n",
    "        \n",
    "        if level1_preds.size == 0:\n",
    "            raise ValueError(\"No valid Level 1 predictions\")\n",
    "        \n",
    "        # Level 2 predictions\n",
    "        level2_preds, _ = self.generate_level1_predictions(self.level2_models, level1_preds)\n",
    "        \n",
    "        # Level 3 final prediction\n",
    "        if self.level3_model and level2_preds.size > 0:\n",
    "            final_preds = self.level3_model.predict(level2_preds)\n",
    "        else:\n",
    "            # Fallback to simple average of Level 1 predictions\n",
    "            final_preds = np.mean(level1_preds, axis=1)\n",
    "        \n",
    "        return final_preds\n",
    "    \n",
    "    def get_prediction_confidence(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate prediction confidence based on model agreement\"\"\"\n",
    "        \n",
    "        level1_preds, _ = self.generate_level1_predictions(self.level1_models, X)\n",
    "        \n",
    "        if level1_preds.size == 0:\n",
    "            return np.zeros(X.shape[0])\n",
    "        \n",
    "        # Calculate confidence as inverse of prediction variance\n",
    "        pred_std = np.std(level1_preds, axis=1)\n",
    "        confidence = 1 / (1 + pred_std)  # Higher confidence for lower variance\n",
    "        \n",
    "        return confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:26:36.721780Z",
     "iopub.status.busy": "2025-07-01T04:26:36.721580Z",
     "iopub.status.idle": "2025-07-01T04:26:36.739388Z",
     "shell.execute_reply": "2025-07-01T04:26:36.738706Z",
     "shell.execute_reply.started": "2025-07-01T04:26:36.721766Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MonteCarloRiskAnalyzer:\n",
    "    \"\"\"Monte Carlo simulation for portfolio risk analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, n_simulations: int = config.MONTE_CARLO_SIMS):\n",
    "        self.n_simulations = n_simulations\n",
    "        self.simulation_results = {}\n",
    "        \n",
    "    def geometric_brownian_motion(self, S0: float, mu: float, sigma: float, \n",
    "                                 T: float, N: int, n_paths: int = 1) -> np.ndarray:\n",
    "        \"\"\"Generate stock price paths using Geometric Brownian Motion\"\"\"\n",
    "        \n",
    "        dt = T / N\n",
    "        \n",
    "        # Generate random shocks\n",
    "        Z = np.random.normal(0, 1, size=(n_paths, N))\n",
    "        \n",
    "        # Initialize price paths\n",
    "        paths = np.zeros((n_paths, N + 1))\n",
    "        paths[:, 0] = S0\n",
    "        \n",
    "        # Generate paths\n",
    "        for t in range(1, N + 1):\n",
    "            paths[:, t] = paths[:, t-1] * np.exp(\n",
    "                (mu - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * Z[:, t-1]\n",
    "            )\n",
    "        \n",
    "        return paths\n",
    "    \n",
    "    def estimate_stock_parameters(self, prices: pd.Series) -> Tuple[float, float]:\n",
    "        \"\"\"Estimate drift and volatility from historical prices\"\"\"\n",
    "        \n",
    "        returns = prices.pct_change().dropna()\n",
    "        \n",
    "        # Annualized parameters\n",
    "        mu = returns.mean() * 252  # Drift\n",
    "        sigma = returns.std() * np.sqrt(252)  # Volatility\n",
    "        \n",
    "        return mu, sigma\n",
    "    \n",
    "    def simulate_portfolio_paths(self, tickers: List[str], weights: np.ndarray,\n",
    "                                predictions: Dict[str, float], \n",
    "                                historical_data: Dict[str, pd.DataFrame],\n",
    "                                time_horizon: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate portfolio paths using Monte Carlo\"\"\"\n",
    "        \n",
    "        print(\"Running Monte Carlo portfolio simulation...\")\n",
    "        \n",
    "        portfolio_paths = np.zeros((self.n_simulations, time_horizon + 1))\n",
    "        portfolio_paths[:, 0] = 1.0  # Normalized initial value\n",
    "        \n",
    "        individual_paths = {}\n",
    "        \n",
    "        for i, ticker in enumerate(tickers):\n",
    "            if ticker not in historical_data:\n",
    "                continue\n",
    "                \n",
    "            # Get current price and estimate parameters\n",
    "            current_price = historical_data[ticker]['Close'].iloc[-1]\n",
    "            mu, sigma = self.estimate_stock_parameters(historical_data[ticker]['Close'])\n",
    "            \n",
    "            # Adjust drift based on prediction\n",
    "            if ticker in predictions:\n",
    "                # Incorporate prediction into drift\n",
    "                predicted_return = predictions[ticker]\n",
    "                adjusted_mu = mu * 0.7 + predicted_return * 252 * 0.3  # Blend historical and predicted\n",
    "            else:\n",
    "                adjusted_mu = mu\n",
    "            \n",
    "            # Generate paths for this stock\n",
    "            stock_paths = self.geometric_brownian_motion(\n",
    "                S0=current_price,\n",
    "                mu=adjusted_mu,\n",
    "                sigma=sigma,\n",
    "                T=time_horizon/252,  # Convert days to years\n",
    "                N=time_horizon,\n",
    "                n_paths=self.n_simulations\n",
    "            )\n",
    "            \n",
    "            # Convert to returns and weight\n",
    "            stock_returns = stock_paths[:, -1] / stock_paths[:, 0] - 1\n",
    "            individual_paths[ticker] = stock_returns\n",
    "            \n",
    "            # Add to portfolio\n",
    "            portfolio_paths[:, -1] += weights[i] * stock_returns\n",
    "        \n",
    "        # Calculate portfolio statistics\n",
    "        final_returns = portfolio_paths[:, -1]\n",
    "        \n",
    "        results = {\n",
    "            'final_returns': final_returns,\n",
    "            'individual_paths': individual_paths,\n",
    "            'mean_return': np.mean(final_returns),\n",
    "            'std_return': np.std(final_returns),\n",
    "            'var_95': np.percentile(final_returns, 5),  # 95% VaR\n",
    "            'var_99': np.percentile(final_returns, 1),  # 99% VaR\n",
    "            'prob_loss': np.mean(final_returns < 0),\n",
    "            'max_loss': np.min(final_returns),\n",
    "            'max_gain': np.max(final_returns)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_simulation_results(self, results: Dict[str, Any], save_path: str = None):\n",
    "        \"\"\"Plot Monte Carlo simulation results\"\"\"\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Histogram of final returns\n",
    "        ax1.hist(results['final_returns'], bins=50, alpha=0.7, density=True)\n",
    "        ax1.axvline(results['var_95'], color='red', linestyle='--', \n",
    "                   label=f\"95% VaR: {results['var_95']:.3f}\")\n",
    "        ax1.axvline(0, color='orange', linestyle='-', alpha=0.7, label='Break-even')\n",
    "        ax1.set_xlabel('Portfolio Return')\n",
    "        ax1.set_ylabel('Density')\n",
    "        ax1.set_title('Distribution of Portfolio Returns')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Risk metrics text\n",
    "        risk_text = f\"\"\"\n",
    "        Mean Return: {results['mean_return']:.3f}\n",
    "        Std Return: {results['std_return']:.3f}\n",
    "        95% VaR: {results['var_95']:.3f}\n",
    "        99% VaR: {results['var_99']:.3f}\n",
    "        Prob of Loss: {results['prob_loss']:.3f}\n",
    "        Max Loss: {results['max_loss']:.3f}\n",
    "        Max Gain: {results['max_gain']:.3f}\n",
    "        \"\"\"\n",
    "        \n",
    "        ax2.text(0.1, 0.5, risk_text, transform=ax2.transAxes, fontsize=12,\n",
    "                verticalalignment='center', bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "        ax2.set_xlim(0, 1)\n",
    "        ax2.set_ylim(0, 1)\n",
    "        ax2.axis('off')\n",
    "        ax2.set_title('Risk Metrics')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:26:36.741490Z",
     "iopub.status.busy": "2025-07-01T04:26:36.741265Z",
     "iopub.status.idle": "2025-07-01T04:26:36.760115Z",
     "shell.execute_reply": "2025-07-01T04:26:36.759503Z",
     "shell.execute_reply.started": "2025-07-01T04:26:36.741448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PortfolioOptimizer:\n",
    "    \"\"\"Improved portfolio optimization using predictions and risk constraints\"\"\"\n",
    "    \n",
    "    def __init__(self, max_allocation: float = config.PORTFOLIO_ALLOCATION,\n",
    "                 max_position_size: float = config.MAX_POSITION_SIZE):\n",
    "        self.max_allocation = max_allocation\n",
    "        self.max_position_size = max_position_size\n",
    "        \n",
    "    def calculate_position_sizes(self, predictions: Dict[str, float],\n",
    "                               confidences: Dict[str, float],\n",
    "                               risk_metrics: Dict[str, Any] = None) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Calculate position sizes ensuring 90% portfolio allocation\"\"\"\n",
    "        \n",
    "        positions = {}\n",
    "        \n",
    "        # Filter predictions by minimum confidence (lower threshold for more positions)\n",
    "        min_confidence = max(0.3, config.MIN_CONFIDENCE - 0.3)  # Lower minimum confidence\n",
    "        filtered_predictions = {\n",
    "            ticker: pred for ticker, pred in predictions.items()\n",
    "            if confidences.get(ticker, 0) >= min_confidence\n",
    "        }\n",
    "        \n",
    "        if not filtered_predictions:\n",
    "            print(f\"No predictions meet minimum confidence threshold of {min_confidence}\")\n",
    "            return positions\n",
    "        \n",
    "        print(f\"Using {len(filtered_predictions)} stocks with confidence >= {min_confidence}\")\n",
    "        \n",
    "        # Separate long and short positions\n",
    "        long_positions = {k: v for k, v in filtered_predictions.items() if v > 0}\n",
    "        short_positions = {k: abs(v) for k, v in filtered_predictions.items() if v < 0}\n",
    "        \n",
    "        print(f\"Long positions: {len(long_positions)}, Short positions: {len(short_positions)}\")\n",
    "        \n",
    "        # Calculate weights ensuring we use the full allocation\n",
    "        def calculate_weights(pos_dict: Dict[str, float], target_allocation: float) -> Dict[str, float]:\n",
    "            if not pos_dict:\n",
    "                return {}\n",
    "            \n",
    "            weights = {}\n",
    "            \n",
    "            # Calculate raw scores\n",
    "            for ticker, pred in pos_dict.items():\n",
    "                confidence = confidences.get(ticker, 0.5)\n",
    "                # Enhanced scoring: prediction magnitude * confidence^2 for better differentiation\n",
    "                score = abs(pred) * (confidence ** 1.5)\n",
    "                weights[ticker] = score\n",
    "            \n",
    "            # Normalize to target allocation\n",
    "            total_score = sum(weights.values())\n",
    "            if total_score > 0:\n",
    "                for ticker in weights:\n",
    "                    # Scale to target allocation\n",
    "                    raw_weight = (weights[ticker] / total_score) * target_allocation\n",
    "                    # Apply position size limits but ensure minimum allocation\n",
    "                    min_weight = target_allocation / (len(pos_dict) * 10)  # Minimum 1/10th of equal weight\n",
    "                    weights[ticker] = max(min_weight, min(raw_weight, self.max_position_size))\n",
    "            \n",
    "            # Rescale to exactly hit target allocation\n",
    "            current_total = sum(weights.values())\n",
    "            if current_total > 0:\n",
    "                scale_factor = target_allocation / current_total\n",
    "                for ticker in weights:\n",
    "                    weights[ticker] *= scale_factor\n",
    "            \n",
    "            return weights\n",
    "        \n",
    "        # Allocate 45% to longs, 45% to shorts (90% total)\n",
    "        long_allocation = self.max_allocation / 2\n",
    "        short_allocation = self.max_allocation / 2\n",
    "        \n",
    "        # If we have more longs than shorts (or vice versa), adjust allocation\n",
    "        if len(long_positions) == 0 and len(short_positions) > 0:\n",
    "            short_allocation = self.max_allocation\n",
    "        elif len(short_positions) == 0 and len(long_positions) > 0:\n",
    "            long_allocation = self.max_allocation\n",
    "        elif len(long_positions) > 0 and len(short_positions) > 0:\n",
    "            # Adjust based on relative strength of predictions\n",
    "            long_strength = sum(abs(v) * confidences.get(k, 0.5) for k, v in long_positions.items())\n",
    "            short_strength = sum(abs(v) * confidences.get(k, 0.5) for k, v in short_positions.items())\n",
    "            total_strength = long_strength + short_strength\n",
    "            \n",
    "            if total_strength > 0:\n",
    "                long_allocation = self.max_allocation * (long_strength / total_strength)\n",
    "                short_allocation = self.max_allocation * (short_strength / total_strength)\n",
    "        \n",
    "        # Calculate weights\n",
    "        long_weights = calculate_weights(long_positions, long_allocation)\n",
    "        short_weights = calculate_weights(short_positions, short_allocation)\n",
    "        \n",
    "        # Combine positions\n",
    "        for ticker, weight in long_weights.items():\n",
    "            positions[ticker] = {\n",
    "                'side': 'buy',\n",
    "                'weight': weight,\n",
    "                'prediction': predictions[ticker],\n",
    "                'confidence': confidences[ticker]\n",
    "            }\n",
    "        \n",
    "        for ticker, weight in short_weights.items():\n",
    "            positions[ticker] = {\n",
    "                'side': 'sell',\n",
    "                'weight': weight,\n",
    "                'prediction': predictions[ticker],\n",
    "                'confidence': confidences[ticker]\n",
    "            }\n",
    "        \n",
    "        # Verify total allocation\n",
    "        total_allocation = sum(pos['weight'] for pos in positions.values())\n",
    "        print(f\"Total portfolio allocation: {total_allocation:.4f} (target: {self.max_allocation})\")\n",
    "        \n",
    "        # If we're still under-allocated, scale up proportionally\n",
    "        if total_allocation < self.max_allocation * 0.8:  # If less than 80% of target\n",
    "            scale_factor = self.max_allocation / max(total_allocation, 0.01)\n",
    "            for ticker in positions:\n",
    "                positions[ticker]['weight'] *= scale_factor\n",
    "            \n",
    "            total_allocation = sum(pos['weight'] for pos in positions.values())\n",
    "            print(f\"Scaled total portfolio allocation: {total_allocation:.4f}\")\n",
    "        \n",
    "        return positions\n",
    "    \n",
    "    def optimize_portfolio_risk(self, positions: Dict[str, Dict[str, float]],\n",
    "                               historical_data: Dict[str, pd.DataFrame]) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Optimize portfolio for risk-adjusted returns\"\"\"\n",
    "        \n",
    "        if not positions:\n",
    "            return positions\n",
    "        \n",
    "        tickers = list(positions.keys())\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        returns_data = []\n",
    "        for ticker in tickers:\n",
    "            if ticker in historical_data:\n",
    "                returns = historical_data[ticker]['Close'].pct_change().dropna()\n",
    "                returns_data.append(returns)\n",
    "        \n",
    "        if len(returns_data) < 2:\n",
    "            return positions  # Can't optimize with less than 2 assets\n",
    "        \n",
    "        # Align returns data\n",
    "        returns_df = pd.concat(returns_data, axis=1, keys=tickers).dropna()\n",
    "        correlation_matrix = returns_df.corr()\n",
    "        \n",
    "        # Simple risk adjustment: reduce weights for highly correlated positions\n",
    "        adjusted_positions = positions.copy()\n",
    "        \n",
    "        for i, ticker1 in enumerate(tickers):\n",
    "            for j, ticker2 in enumerate(tickers):\n",
    "                if i < j and ticker1 in correlation_matrix.index and ticker2 in correlation_matrix.columns:\n",
    "                    corr = abs(correlation_matrix.loc[ticker1, ticker2])\n",
    "                    \n",
    "                    if corr > 0.7:  # High correlation\n",
    "                        # Reduce weights proportionally\n",
    "                        reduction_factor = 1 - (corr - 0.7) * 0.5\n",
    "                        adjusted_positions[ticker1]['weight'] *= reduction_factor\n",
    "                        adjusted_positions[ticker2]['weight'] *= reduction_factor\n",
    "        \n",
    "        return adjusted_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:26:36.782403Z",
     "iopub.status.busy": "2025-07-01T04:26:36.782152Z",
     "iopub.status.idle": "2025-07-01T04:26:36.805711Z",
     "shell.execute_reply": "2025-07-01T04:26:36.805015Z",
     "shell.execute_reply.started": "2025-07-01T04:26:36.782375Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "    \"\"\"Handles saving and loading of trained models\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str = config.MODEL_SAVE_PATH):\n",
    "        self.base_path = base_path\n",
    "        os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    def save_ensemble_model(self, ensemble: EnsembleStacker, \n",
    "                           metadata: Dict[str, Any], \n",
    "                           model_name: str = \"ensemble_model\") -> str:\n",
    "        \"\"\"Save the complete ensemble model\"\"\"\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_dir = os.path.join(self.base_path, f\"{model_name}_{timestamp}\")\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Save ensemble object\n",
    "        ensemble_path = os.path.join(model_dir, \"ensemble.pkl\")\n",
    "        with open(ensemble_path, 'wb') as f:\n",
    "            pickle.dump(ensemble, f)\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata_path = os.path.join(model_dir, \"metadata.json\")\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2, default=str)\n",
    "        \n",
    "        # Save individual PyTorch models separately\n",
    "        torch_models = {}\n",
    "        for name, model in ensemble.level1_models.items():\n",
    "            if isinstance(model, MLPTrainer):\n",
    "                torch_path = os.path.join(model_dir, f\"{name}_torch.pth\")\n",
    "                torch.save(model.model.state_dict(), torch_path)\n",
    "                torch_models[name] = torch_path\n",
    "        \n",
    "        for name, model in ensemble.level2_models.items():\n",
    "            if isinstance(model, MLPTrainer):\n",
    "                torch_path = os.path.join(model_dir, f\"{name}_torch.pth\")\n",
    "                torch.save(model.model.state_dict(), torch_path)\n",
    "                torch_models[name] = torch_path\n",
    "        \n",
    "        # Save torch model paths\n",
    "        if torch_models:\n",
    "            torch_paths_file = os.path.join(model_dir, \"torch_models.json\")\n",
    "            with open(torch_paths_file, 'w') as f:\n",
    "                json.dump(torch_models, f, indent=2)\n",
    "        \n",
    "        print(f\"Model saved to: {model_dir}\")\n",
    "        return model_dir\n",
    "    \n",
    "    def load_ensemble_model(self, model_path: str) -> Tuple[EnsembleStacker, Dict[str, Any]]:\n",
    "        \"\"\"Load a saved ensemble model\"\"\"\n",
    "        \n",
    "        # Load ensemble object\n",
    "        ensemble_path = os.path.join(model_path, \"ensemble.pkl\")\n",
    "        with open(ensemble_path, 'rb') as f:\n",
    "            ensemble = pickle.load(f)\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata_path = os.path.join(model_path, \"metadata.json\")\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        # Load PyTorch models if they exist\n",
    "        torch_paths_file = os.path.join(model_path, \"torch_models.json\")\n",
    "        if os.path.exists(torch_paths_file):\n",
    "            with open(torch_paths_file, 'r') as f:\n",
    "                torch_models = json.load(f)\n",
    "            \n",
    "            # Reload PyTorch model states\n",
    "            for name, torch_path in torch_models.items():\n",
    "                if name in ensemble.level1_models and isinstance(ensemble.level1_models[name], MLPTrainer):\n",
    "                    ensemble.level1_models[name].model.load_state_dict(torch.load(torch_path, map_location=config.DEVICE))\n",
    "                elif name in ensemble.level2_models and isinstance(ensemble.level2_models[name], MLPTrainer):\n",
    "                    ensemble.level2_models[name].model.load_state_dict(torch.load(torch_path, map_location=config.DEVICE))\n",
    "        \n",
    "        print(f\"Model loaded from: {model_path}\")\n",
    "        return ensemble, metadata\n",
    "    \n",
    "    def list_saved_models(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"List all saved models with their metadata\"\"\"\n",
    "        \n",
    "        models = []\n",
    "        \n",
    "        for item in os.listdir(self.base_path):\n",
    "            item_path = os.path.join(self.base_path, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                metadata_path = os.path.join(item_path, \"metadata.json\")\n",
    "                if os.path.exists(metadata_path):\n",
    "                    try:\n",
    "                        with open(metadata_path, 'r') as f:\n",
    "                            metadata = json.load(f)\n",
    "                        \n",
    "                        models.append({\n",
    "                            'name': item,\n",
    "                            'path': item_path,\n",
    "                            'metadata': metadata\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not load metadata for {item}: {e}\")\n",
    "        \n",
    "        return sorted(models, key=lambda x: x['name'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:26:36.806862Z",
     "iopub.status.busy": "2025-07-01T04:26:36.806593Z",
     "iopub.status.idle": "2025-07-01T04:26:36.841714Z",
     "shell.execute_reply": "2025-07-01T04:26:36.841161Z",
     "shell.execute_reply.started": "2025-07-01T04:26:36.806841Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class StockPredictionTradingSystem:\n",
    "    \"\"\"Main system orchestrating the entire pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, tickers_file: str = \"tickers.txt\"):\n",
    "        # Load tickers\n",
    "        with open(tickers_file, 'r') as f:\n",
    "            self.tickers = [line.strip() for line in f.readlines() if line.strip()]\n",
    "        \n",
    "        print(f\"Loaded {len(self.tickers)} tickers\")\n",
    "        \n",
    "        # Initialize components\n",
    "        self.data_collector = DataCollector(self.tickers)\n",
    "        self.ensemble = EnsembleStacker()\n",
    "        self.risk_analyzer = MonteCarloRiskAnalyzer()\n",
    "        self.portfolio_optimizer = PortfolioOptimizer()\n",
    "        self.model_manager = ModelManager()\n",
    "        \n",
    "        # Data storage\n",
    "        self.historical_data = {}\n",
    "        self.predictions = {}\n",
    "        self.confidences = {}\n",
    "        self.portfolio_positions = {}\n",
    "        \n",
    "    def collect_and_prepare_data(self) -> bool:\n",
    "        \"\"\"Collect and prepare all data for training\"\"\"\n",
    "        \n",
    "        print(\"=== DATA COLLECTION PHASE ===\")\n",
    "        \n",
    "        # Collect historical data\n",
    "        self.historical_data = self.data_collector.collect_all_data()\n",
    "        \n",
    "        if not self.historical_data:\n",
    "            print(\"No data collected. Exiting.\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"Successfully collected data for {len(self.historical_data)} stocks\")\n",
    "        \n",
    "        # Save raw data\n",
    "        data_file = os.path.join(config.DATA_SAVE_PATH, f\"historical_data_{datetime.now().strftime('%Y%m%d')}.pkl\")\n",
    "        with open(data_file, 'wb') as f:\n",
    "            pickle.dump(self.historical_data, f)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def train_prediction_models(self) -> bool:\n",
    "        \"\"\"Train individual prediction models for each stock for better accuracy\"\"\"\n",
    "        \n",
    "        print(\"=== MODEL TRAINING PHASE ===\")\n",
    "        \n",
    "        # Train individual models for each stock\n",
    "        self.individual_models = {}\n",
    "        self.individual_scalers = {}\n",
    "        \n",
    "        successful_models = 0\n",
    "        \n",
    "        for ticker in self.tickers:\n",
    "            if ticker not in self.historical_data:\n",
    "                continue\n",
    "                \n",
    "            print(f\"Training model for {ticker}...\")\n",
    "            \n",
    "            try:\n",
    "                X, y = self.data_collector.prepare_features(ticker)\n",
    "                \n",
    "                if X.size == 0 or y.size == 0 or len(X) < 100:  # Need minimum data\n",
    "                    print(f\"Insufficient data for {ticker}\")\n",
    "                    continue\n",
    "                \n",
    "                # Remove any infinite or NaN values\n",
    "                mask = np.isfinite(X).all(axis=1) & np.isfinite(y)\n",
    "                X = X[mask]\n",
    "                y = y[mask]\n",
    "                \n",
    "                if len(X) < 50:\n",
    "                    continue\n",
    "                \n",
    "                # Split data\n",
    "                split_idx = int(len(X) * 0.8)\n",
    "                X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "                y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "                \n",
    "                # Further split training for validation\n",
    "                val_split_idx = int(len(X_train) * 0.8)\n",
    "                X_train_sub, X_val = X_train[:val_split_idx], X_train[val_split_idx:]\n",
    "                y_train_sub, y_val = y_train[:val_split_idx], y_train[val_split_idx:]\n",
    "                \n",
    "                # Create ensemble for this stock\n",
    "                stock_ensemble = EnsembleStacker()\n",
    "                \n",
    "                # Train Level 1 models\n",
    "                level1_models = {}\n",
    "                \n",
    "                # Neural Network\n",
    "                mlp_trainer = MLPTrainer(X_train_sub.shape[1])\n",
    "                mlp_trainer.train(X_train_sub, y_train_sub, X_val, y_val, epochs=150)\n",
    "                level1_models['nn_mlp'] = mlp_trainer\n",
    "                \n",
    "                # XGBoost\n",
    "                xgb_model = xgb.XGBRegressor(\n",
    "                    n_estimators=500,\n",
    "                    max_depth=6,\n",
    "                    learning_rate=0.01,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    random_state=42\n",
    "                )\n",
    "                if config.USE_GPU:\n",
    "                    xgb_model.set_params(tree_method='gpu_hist', gpu_id=0)\n",
    "                \n",
    "                xgb_model.fit(X_train_sub, y_train_sub,\n",
    "                             eval_set=[(X_val, y_val)],\n",
    "                             early_stopping_rounds=50,\n",
    "                             verbose=False)\n",
    "                level1_models['xgboost'] = xgb_model\n",
    "                \n",
    "                # LightGBM\n",
    "                lgb_model = lgb.LGBMRegressor(\n",
    "                    n_estimators=500,\n",
    "                    max_depth=6,\n",
    "                    learning_rate=0.01,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    random_state=42,\n",
    "                    verbose=-1\n",
    "                )\n",
    "                if config.USE_GPU:\n",
    "                    lgb_model.set_params(device='gpu')\n",
    "                \n",
    "                lgb_model.fit(X_train_sub, y_train_sub,\n",
    "                             eval_set=[(X_val, y_val)],\n",
    "                             callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\n",
    "                level1_models['lightgbm'] = lgb_model\n",
    "                \n",
    "                # Random Forest\n",
    "                rf_model = RandomForestRegressor(\n",
    "                    n_estimators=200,\n",
    "                    max_depth=10,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                rf_model.fit(X_train_sub, y_train_sub)\n",
    "                level1_models['random_forest'] = rf_model\n",
    "                \n",
    "                # Store models\n",
    "                stock_ensemble.level1_models = level1_models\n",
    "                \n",
    "                # Generate Level 1 predictions for Level 2 training\n",
    "                level1_train_preds = []\n",
    "                level1_val_preds = []\n",
    "                \n",
    "                for name, model in level1_models.items():\n",
    "                    if name == 'nn_mlp':\n",
    "                        train_pred = model.predict(X_train_sub)\n",
    "                        val_pred = model.predict(X_val)\n",
    "                    else:\n",
    "                        train_pred = model.predict(X_train_sub)\n",
    "                        val_pred = model.predict(X_val)\n",
    "                    \n",
    "                    level1_train_preds.append(train_pred.reshape(-1, 1))\n",
    "                    level1_val_preds.append(val_pred.reshape(-1, 1))\n",
    "                \n",
    "                if level1_train_preds:\n",
    "                    level1_train_combined = np.hstack(level1_train_preds)\n",
    "                    level1_val_combined = np.hstack(level1_val_preds)\n",
    "                    \n",
    "                    # Level 2: Meta-model (simple Ridge regression)\n",
    "                    meta_model = Ridge(alpha=1.0, random_state=42)\n",
    "                    meta_model.fit(level1_train_combined, y_train_sub)\n",
    "                    \n",
    "                    stock_ensemble.level2_models = {'ridge_meta': meta_model}\n",
    "                    stock_ensemble.level3_model = None  # Use Level 2 as final\n",
    "                \n",
    "                # Test the ensemble\n",
    "                test_pred = self._predict_with_stock_ensemble(stock_ensemble, X_test)\n",
    "                test_mse = mean_squared_error(y_test, test_pred)\n",
    "                test_r2 = r2_score(y_test, test_pred)\n",
    "                \n",
    "                print(f\"{ticker} - Test MSE: {test_mse:.6f}, R: {test_r2:.6f}\")\n",
    "                \n",
    "                # Store successful model\n",
    "                self.individual_models[ticker] = stock_ensemble\n",
    "                successful_models += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to train model for {ticker}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Successfully trained models for {successful_models} stocks\")\n",
    "        return successful_models > 0\n",
    "    \n",
    "    def _predict_with_stock_ensemble(self, ensemble, X):\n",
    "        \"\"\"Helper method to predict with individual stock ensemble\"\"\"\n",
    "        \n",
    "        # Level 1 predictions\n",
    "        level1_preds = []\n",
    "        for name, model in ensemble.level1_models.items():\n",
    "            if name == 'nn_mlp':\n",
    "                pred = model.predict(X)\n",
    "            else:\n",
    "                pred = model.predict(X)\n",
    "            level1_preds.append(pred.reshape(-1, 1))\n",
    "        \n",
    "        if not level1_preds:\n",
    "            return np.zeros(X.shape[0])\n",
    "        \n",
    "        level1_combined = np.hstack(level1_preds)\n",
    "        \n",
    "        # Level 2 prediction\n",
    "        if ensemble.level2_models and 'ridge_meta' in ensemble.level2_models:\n",
    "            final_pred = ensemble.level2_models['ridge_meta'].predict(level1_combined)\n",
    "        else:\n",
    "            # Fallback to simple average\n",
    "            final_pred = np.mean(level1_combined, axis=1)\n",
    "        \n",
    "        return final_pred\n",
    "    \n",
    "    def generate_predictions(self) -> bool:\n",
    "        \"\"\"Generate predictions using individual stock models\"\"\"\n",
    "        \n",
    "        print(\"=== PREDICTION GENERATION PHASE ===\")\n",
    "        \n",
    "        if not hasattr(self, 'individual_models'):\n",
    "            print(\"No individual models found. Run training first.\")\n",
    "            return False\n",
    "        \n",
    "        for ticker in self.individual_models.keys():\n",
    "            try:\n",
    "                X, y = self.data_collector.prepare_features(ticker)\n",
    "                \n",
    "                if X.size == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Use the last 10 data points for prediction (more stable)\n",
    "                X_recent = X[-10:] if len(X) >= 10 else X[-1:]\n",
    "                \n",
    "                # Get ensemble prediction\n",
    "                ensemble = self.individual_models[ticker]\n",
    "                predictions_recent = self._predict_with_stock_ensemble(ensemble, X_recent)\n",
    "                \n",
    "                # Use the mean of recent predictions\n",
    "                prediction = np.mean(predictions_recent)\n",
    "                \n",
    "                # Calculate confidence based on prediction consistency\n",
    "                if len(predictions_recent) > 1:\n",
    "                    pred_std = np.std(predictions_recent)\n",
    "                    confidence = 1 / (1 + pred_std * 10)  # Higher confidence for lower variance\n",
    "                else:\n",
    "                    confidence = 0.5\n",
    "                \n",
    "                # Ensure reasonable bounds\n",
    "                prediction = np.clip(prediction, -0.5, 0.5)  # Limit to 50% returns\n",
    "                confidence = np.clip(confidence, 0.1, 0.99)\n",
    "                \n",
    "                self.predictions[ticker] = prediction\n",
    "                self.confidences[ticker] = confidence\n",
    "                \n",
    "                print(f\"{ticker}: Prediction = {prediction:.4f}, Confidence = {confidence:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Could not generate prediction for {ticker}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Generated predictions for {len(self.predictions)} stocks\")\n",
    "        \n",
    "        # Log prediction statistics\n",
    "        if self.predictions:\n",
    "            pred_values = list(self.predictions.values())\n",
    "            conf_values = list(self.confidences.values())\n",
    "            \n",
    "            print(f\"Prediction stats - Mean: {np.mean(pred_values):.4f}, \"\n",
    "                       f\"Std: {np.std(pred_values):.4f}, \"\n",
    "                       f\"Min: {np.min(pred_values):.4f}, \"\n",
    "                       f\"Max: {np.max(pred_values):.4f}\")\n",
    "            print(f\"Confidence stats - Mean: {np.mean(conf_values):.4f}, \"\n",
    "                       f\"Min: {np.min(conf_values):.4f}, \"\n",
    "                       f\"Max: {np.max(conf_values):.4f}\")\n",
    "        \n",
    "        return len(self.predictions) > 0\n",
    "    \n",
    "    def analyze_portfolio_risk(self) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze portfolio risk using Monte Carlo simulation\"\"\"\n",
    "        \n",
    "        print(\"=== RISK ANALYSIS PHASE ===\")\n",
    "        \n",
    "        if not self.predictions:\n",
    "            print(\"No predictions available for risk analysis\")\n",
    "            return {}\n",
    "        \n",
    "        # Get tickers with predictions\n",
    "        predicted_tickers = list(self.predictions.keys())\n",
    "        \n",
    "        # Create equal weights for simulation (will be optimized later)\n",
    "        weights = np.ones(len(predicted_tickers)) / len(predicted_tickers)\n",
    "        \n",
    "        # Run Monte Carlo simulation\n",
    "        risk_results = self.risk_analyzer.simulate_portfolio_paths(\n",
    "            tickers=predicted_tickers,\n",
    "            weights=weights,\n",
    "            predictions=self.predictions,\n",
    "            historical_data=self.historical_data,\n",
    "            time_horizon=config.PREDICTION_HORIZON\n",
    "        )\n",
    "        \n",
    "        # Plot results\n",
    "        plot_path = os.path.join(config.RESULTS_SAVE_PATH, f\"risk_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\")\n",
    "        self.risk_analyzer.plot_simulation_results(risk_results, plot_path)\n",
    "        \n",
    "        print(\"Risk Analysis Results:\")\n",
    "        print(f\"  Expected Return: {risk_results['mean_return']:.4f}\")\n",
    "        print(f\"  Volatility: {risk_results['std_return']:.4f}\")\n",
    "        print(f\"  95% VaR: {risk_results['var_95']:.4f}\")\n",
    "        print(f\"  Probability of Loss: {risk_results['prob_loss']:.4f}\")\n",
    "        \n",
    "        return risk_results\n",
    "    \n",
    "    def optimize_portfolio(self, risk_results: Dict[str, Any] = None) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Optimize portfolio allocation\"\"\"\n",
    "        \n",
    "        print(\"=== PORTFOLIO OPTIMIZATION PHASE ===\")\n",
    "        \n",
    "        # Calculate position sizes\n",
    "        positions = self.portfolio_optimizer.calculate_position_sizes(\n",
    "            self.predictions, self.confidences, risk_results\n",
    "        )\n",
    "        \n",
    "        # Risk-adjust positions\n",
    "        optimized_positions = self.portfolio_optimizer.optimize_portfolio_risk(\n",
    "            positions, self.historical_data\n",
    "        )\n",
    "        \n",
    "        self.portfolio_positions = optimized_positions\n",
    "        \n",
    "        print(f\"Portfolio Optimization Results:\")\n",
    "        print(f\"  Number of positions: {len(optimized_positions)}\")\n",
    "        \n",
    "        total_long_weight = sum(pos['weight'] for pos in optimized_positions.values() if pos['side'] == 'buy')\n",
    "        total_short_weight = sum(pos['weight'] for pos in optimized_positions.values() if pos['side'] == 'sell')\n",
    "        \n",
    "        print(f\"  Total long weight: {total_long_weight:.4f}\")\n",
    "        print(f\"  Total short weight: {total_short_weight:.4f}\")\n",
    "        print(f\"  Net exposure: {total_long_weight - total_short_weight:.4f}\")\n",
    "        \n",
    "        return optimized_positions\n",
    "    \n",
    "    def run_complete_pipeline(self, dry_run: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Run the complete pipeline from data collection to trading\"\"\"\n",
    "        \n",
    "        print(\"=== STARTING COMPLETE PIPELINE ===\")\n",
    "        \n",
    "        results = {\n",
    "            'start_time': datetime.now().isoformat(),\n",
    "            'success': False,\n",
    "            'phases_completed': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Phase 1: Data Collection\n",
    "            if self.collect_and_prepare_data():\n",
    "                results['phases_completed'].append('data_collection')\n",
    "            else:\n",
    "                return results\n",
    "            \n",
    "            # Phase 2: Model Training\n",
    "            if self.train_prediction_models():\n",
    "                results['phases_completed'].append('model_training')\n",
    "            else:\n",
    "                return results\n",
    "            \n",
    "            # Phase 3: Generate Predictions\n",
    "            if self.generate_predictions():\n",
    "                results['phases_completed'].append('prediction_generation')\n",
    "                results['predictions'] = self.predictions\n",
    "                results['confidences'] = self.confidences\n",
    "            else:\n",
    "                return results\n",
    "            \n",
    "            # Phase 4: Risk Analysis\n",
    "            risk_results = self.analyze_portfolio_risk()\n",
    "            if risk_results:\n",
    "                results['phases_completed'].append('risk_analysis')\n",
    "                results['risk_analysis'] = risk_results\n",
    "\n",
    "            # Phase 5: Portfolio Optimization\n",
    "            optimized_positions = self.optimize_portfolio(risk_results)\n",
    "            if optimized_positions:\n",
    "                results['phases_completed'].append('portfolio_optimization')\n",
    "                results['portfolio_positions'] = optimized_positions\n",
    "            \n",
    "            results['success'] = True\n",
    "            results['end_time'] = datetime.now().isoformat()\n",
    "            \n",
    "            print(\"=== PIPELINE COMPLETED SUCCESSFULLY ===\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Pipeline failed: {e}\")\n",
    "            results['error'] = str(e)\n",
    "            results['end_time'] = datetime.now().isoformat()\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:26:36.863285Z",
     "iopub.status.busy": "2025-07-01T04:26:36.863060Z",
     "iopub.status.idle": "2025-07-01T04:29:54.287845Z",
     "shell.execute_reply": "2025-07-01T04:29:54.287225Z",
     "shell.execute_reply.started": "2025-07-01T04:26:36.863272Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78 tickers\n",
      "Connected to Alpaca. Account status: ACTIVE\n",
      "Buying power: $200,000.00\n",
      "=== STARTING COMPLETE PIPELINE ===\n",
      "=== DATA COLLECTION PHASE ===\n",
      "Starting data collection...\n",
      "Processing ANET...\n",
      "Processing GOOG...\n",
      "Processing AEE...\n",
      "Processing ACN...\n",
      "Processing AAPL...\n",
      "Processing JPM...\n",
      "Processing ADM...\n",
      "Processing ADBE...\n",
      "Processing AEP...\n",
      "Processing ALB...\n",
      "Processing APTV...\n",
      "Processing MCD...\n",
      "Processing APO...\n",
      "Processing ARE...\n",
      "Processing T...\n",
      "Processing TSLA...\n",
      "Processing AMP...\n",
      "Processing GS...\n",
      "Processing ALL...\n",
      "Processing AES...\n",
      "Processing ANSS...\n",
      "Processing JNJ...\n",
      "Processing UNH...\n",
      "Processing AMCR...\n",
      "Processing NVDA...\n",
      "Processing AWK...\n",
      "Processing A...\n",
      "Processing MMM...\n",
      "Processing GOOGL...\n",
      "Processing AMAT...\n",
      "Processing BA...\n",
      "Processing KO...\n",
      "Processing ABT...\n",
      "Processing CVX...\n",
      "Processing APA...\n",
      "Processing AXP...\n",
      "Processing MO...\n",
      "Processing PFE...\n",
      "Processing COST...\n",
      "Processing APH...\n",
      "Processing LNT...\n",
      "Processing NKE...\n",
      "Processing XOM...\n",
      "Processing ALLE...\n",
      "Processing AIZ...\n",
      "Processing ADSK...\n",
      "Processing ACGL...\n",
      "Processing WFC...\n",
      "Processing AME...\n",
      "Processing AOS...\n",
      "Processing AMD...\n",
      "Processing AMGN...\n",
      "Processing AIG...\n",
      "Processing WMT...\n",
      "Processing AMZN...\n",
      "Processing AON...\n",
      "Processing AKAM...\n",
      "Processing AFL...\n",
      "Processing TMO...\n",
      "Processing MS...\n",
      "Processing AJG...\n",
      "Processing META...\n",
      "Processing PG...\n",
      "Processing V...\n",
      "Processing MA...\n",
      "Processing AMT...\n",
      "Processing BAC...\n",
      "Processing ALGN...\n",
      "Processing ATO...\n",
      "Processing CAT...\n",
      "Processing HD...\n",
      "Processing ADI...\n",
      "Processing ABBV...\n",
      "Processing ABNB...\n",
      "Processing APD...\n",
      "Processing MRK...\n",
      "Processing MSFT...\n",
      "Processing GE...\n",
      "Data collection completed for 78 tickers\n",
      "Successfully collected data for 78 stocks\n",
      "=== MODEL TRAINING PHASE ===\n",
      "Training model for ANET...\n",
      "Epoch 0: Train Loss = 1.665744, Val Loss = 0.955276, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's l2: 0.00448618\n",
      "ANET - Test MSE: 0.009091, R: -0.214519\n",
      "Training model for GOOG...\n",
      "Epoch 0: Train Loss = 1.754613, Val Loss = 0.837412, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's l2: 0.00180081\n",
      "GOOG - Test MSE: 0.001894, R: 0.122707\n",
      "Training model for AEE...\n",
      "Epoch 0: Train Loss = 2.590396, Val Loss = 0.964502, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.00075016\n",
      "AEE - Test MSE: 0.000556, R: 0.032200\n",
      "Training model for ACN...\n",
      "Epoch 0: Train Loss = 1.791795, Val Loss = 0.507185, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.000715074\n",
      "ACN - Test MSE: 0.001399, R: 0.072263\n",
      "Training model for AAPL...\n",
      "Epoch 0: Train Loss = 1.748930, Val Loss = 0.670755, LR = 0.000994\n",
      "Early stopping at epoch 21\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.00118005\n",
      "AAPL - Test MSE: 0.003481, R: 0.012683\n",
      "Training model for JPM...\n",
      "Epoch 0: Train Loss = 2.139513, Val Loss = 1.283600, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.00100353\n",
      "JPM - Test MSE: 0.002456, R: 0.002291\n",
      "Training model for ADM...\n",
      "Epoch 0: Train Loss = 2.430034, Val Loss = 0.923945, LR = 0.000994\n",
      "Early stopping at epoch 22\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.00126422\n",
      "ADM - Test MSE: 0.002446, R: -0.052964\n",
      "Training model for ADBE...\n",
      "Epoch 0: Train Loss = 2.046269, Val Loss = 1.502823, LR = 0.000994\n",
      "Early stopping at epoch 34\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's l2: 0.00324583\n",
      "ADBE - Test MSE: 0.004416, R: -0.865996\n",
      "Training model for AEP...\n",
      "Epoch 0: Train Loss = 1.986368, Val Loss = 0.639361, LR = 0.000994\n",
      "Early stopping at epoch 27\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's l2: 0.000731831\n",
      "AEP - Test MSE: 0.000748, R: 0.032570\n",
      "Training model for ALB...\n",
      "Epoch 0: Train Loss = 1.674236, Val Loss = 0.392379, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.00374475\n",
      "ALB - Test MSE: 0.005833, R: 0.045974\n",
      "Training model for APTV...\n",
      "Epoch 0: Train Loss = 2.314112, Val Loss = 0.986633, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[108]\tvalid_0's l2: 0.00194249\n",
      "APTV - Test MSE: 0.003880, R: -0.003867\n",
      "Training model for MCD...\n",
      "Epoch 0: Train Loss = 1.581301, Val Loss = 0.504480, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's l2: 0.000582758\n",
      "MCD - Test MSE: 0.000561, R: 0.051926\n",
      "Training model for APO...\n",
      "Epoch 0: Train Loss = 2.479283, Val Loss = 1.302482, LR = 0.000994\n",
      "Early stopping at epoch 32\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's l2: 0.00244866\n",
      "APO - Test MSE: 0.006039, R: -0.064398\n",
      "Training model for ARE...\n",
      "Epoch 0: Train Loss = 2.144150, Val Loss = 0.744630, LR = 0.000994\n",
      "Early stopping at epoch 23\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's l2: 0.00125468\n",
      "ARE - Test MSE: 0.003607, R: -0.807842\n",
      "Training model for T...\n",
      "Epoch 0: Train Loss = 1.830674, Val Loss = 0.901833, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.000840565\n",
      "T - Test MSE: 0.000771, R: 0.020952\n",
      "Training model for TSLA...\n",
      "Epoch 0: Train Loss = 2.435226, Val Loss = 2.281797, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.011873\n",
      "TSLA - Test MSE: 0.012421, R: -0.013347\n",
      "Training model for AMP...\n",
      "Epoch 0: Train Loss = 2.213708, Val Loss = 1.438492, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's l2: 0.00141052\n",
      "AMP - Test MSE: 0.002290, R: -0.063904\n",
      "Training model for GS...\n",
      "Epoch 0: Train Loss = 1.842079, Val Loss = 1.828774, LR = 0.000994\n",
      "Early stopping at epoch 21\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's l2: 0.00231768\n",
      "GS - Test MSE: 0.003659, R: -0.012919\n",
      "Training model for ALL...\n",
      "Epoch 0: Train Loss = 1.839327, Val Loss = 0.588553, LR = 0.000994\n",
      "Early stopping at epoch 23\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[103]\tvalid_0's l2: 0.000861852\n",
      "ALL - Test MSE: 0.001245, R: 0.077847\n",
      "Training model for AES...\n",
      "Epoch 0: Train Loss = 2.131332, Val Loss = 0.439680, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.00295881\n",
      "AES - Test MSE: 0.009057, R: -0.197198\n",
      "Training model for ANSS...\n",
      "Epoch 0: Train Loss = 1.835529, Val Loss = 0.529503, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's l2: 0.000784628\n",
      "ANSS - Test MSE: 0.001187, R: 0.057335\n",
      "Training model for JNJ...\n",
      "Epoch 0: Train Loss = 1.797258, Val Loss = 0.392670, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[149]\tvalid_0's l2: 0.00037967\n",
      "JNJ - Test MSE: 0.000750, R: 0.004452\n",
      "Training model for UNH...\n",
      "Epoch 0: Train Loss = 2.427363, Val Loss = 3.034492, LR = 0.000994\n",
      "Epoch 50: Train Loss = 0.668091, Val Loss = 2.855304, LR = 0.000121\n",
      "Early stopping at epoch 54\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[53]\tvalid_0's l2: 0.00247843\n",
      "UNH - Test MSE: 0.010336, R: -0.206053\n",
      "Training model for AMCR...\n",
      "Epoch 0: Train Loss = 1.349830, Val Loss = 0.818710, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's l2: 0.000932281\n",
      "AMCR - Test MSE: 0.001071, R: -0.093736\n",
      "Training model for NVDA...\n",
      "Epoch 0: Train Loss = 2.445729, Val Loss = 0.548385, LR = 0.000994\n",
      "Early stopping at epoch 49\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[73]\tvalid_0's l2: 0.00442279\n",
      "NVDA - Test MSE: 0.005442, R: 0.105148\n",
      "Training model for AWK...\n",
      "Epoch 0: Train Loss = 2.323696, Val Loss = 0.576326, LR = 0.000994\n",
      "Early stopping at epoch 24\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid_0's l2: 0.00055984\n",
      "AWK - Test MSE: 0.001150, R: 0.061242\n",
      "Training model for A...\n",
      "Epoch 0: Train Loss = 1.780989, Val Loss = 1.018299, LR = 0.000994\n",
      "Early stopping at epoch 24\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[124]\tvalid_0's l2: 0.00131425\n",
      "A - Test MSE: 0.002082, R: -0.103574\n",
      "Training model for MMM...\n",
      "Epoch 0: Train Loss = 3.034729, Val Loss = 0.903869, LR = 0.000994\n",
      "Early stopping at epoch 24\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.000998055\n",
      "MMM - Test MSE: 0.001899, R: 0.010909\n",
      "Training model for GOOGL...\n",
      "Epoch 0: Train Loss = 1.932121, Val Loss = 0.940074, LR = 0.000994\n",
      "Early stopping at epoch 21\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's l2: 0.00191731\n",
      "GOOGL - Test MSE: 0.001914, R: 0.132673\n",
      "Training model for AMAT...\n",
      "Epoch 0: Train Loss = 2.377411, Val Loss = 0.727288, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's l2: 0.00298191\n",
      "AMAT - Test MSE: 0.004231, R: -0.135231\n",
      "Training model for BA...\n",
      "Epoch 0: Train Loss = 1.811770, Val Loss = 0.473323, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's l2: 0.00191441\n",
      "BA - Test MSE: 0.006025, R: -0.063368\n",
      "Training model for KO...\n",
      "Epoch 0: Train Loss = 4.476453, Val Loss = 1.565907, LR = 0.000994\n",
      "Early stopping at epoch 21\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's l2: 0.000776269\n",
      "KO - Test MSE: 0.000485, R: 0.014261\n",
      "Training model for ABT...\n",
      "Epoch 0: Train Loss = 2.949727, Val Loss = 0.962432, LR = 0.000994\n",
      "Early stopping at epoch 21\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's l2: 0.00102644\n",
      "ABT - Test MSE: 0.000983, R: -0.151742\n",
      "Training model for CVX...\n",
      "Epoch 0: Train Loss = 2.204902, Val Loss = 0.978213, LR = 0.000994\n",
      "Early stopping at epoch 21\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's l2: 0.00114011\n",
      "CVX - Test MSE: 0.002031, R: 0.048604\n",
      "Training model for APA...\n",
      "Epoch 0: Train Loss = 1.813469, Val Loss = 0.553733, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.0025216\n",
      "APA - Test MSE: 0.009202, R: 0.021518\n",
      "Training model for AXP...\n",
      "Epoch 0: Train Loss = 1.665242, Val Loss = 0.752423, LR = 0.000994\n",
      "Early stopping at epoch 25\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's l2: 0.000985857\n",
      "AXP - Test MSE: 0.003003, R: -0.040088\n",
      "Training model for MO...\n",
      "Epoch 0: Train Loss = 1.899867, Val Loss = 0.617895, LR = 0.000994\n",
      "Early stopping at epoch 22\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's l2: 0.000507147\n",
      "MO - Test MSE: 0.000777, R: -0.024808\n",
      "Training model for PFE...\n",
      "Epoch 0: Train Loss = 1.670218, Val Loss = 0.496935, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid_0's l2: 0.000769405\n",
      "PFE - Test MSE: 0.001546, R: -0.020238\n",
      "Training model for COST...\n",
      "Epoch 0: Train Loss = 1.762251, Val Loss = 1.035612, LR = 0.000994\n",
      "Early stopping at epoch 43\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's l2: 0.000982373\n",
      "COST - Test MSE: 0.001403, R: -0.002237\n",
      "Training model for APH...\n",
      "Epoch 0: Train Loss = 2.951798, Val Loss = 1.473800, LR = 0.000994\n",
      "Early stopping at epoch 27\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's l2: 0.00188084\n",
      "APH - Test MSE: 0.004032, R: -0.270335\n",
      "Training model for LNT...\n",
      "Epoch 0: Train Loss = 2.178181, Val Loss = 0.828797, LR = 0.000994\n",
      "Early stopping at epoch 22\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's l2: 0.000850954\n",
      "LNT - Test MSE: 0.000676, R: 0.001927\n",
      "Training model for NKE...\n",
      "Epoch 0: Train Loss = 2.047604, Val Loss = 0.668350, LR = 0.000994\n",
      "Early stopping at epoch 26\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid_0's l2: 0.00132633\n",
      "NKE - Test MSE: 0.004254, R: -0.148137\n",
      "Training model for XOM...\n",
      "Epoch 0: Train Loss = 2.104720, Val Loss = 0.373045, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's l2: 0.000633006\n",
      "XOM - Test MSE: 0.001990, R: 0.007410\n",
      "Training model for ALLE...\n",
      "Epoch 0: Train Loss = 1.617881, Val Loss = 0.413454, LR = 0.000994\n",
      "Early stopping at epoch 45\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[109]\tvalid_0's l2: 0.000618645\n",
      "ALLE - Test MSE: 0.001345, R: 0.001656\n",
      "Training model for AIZ...\n",
      "Epoch 0: Train Loss = 1.911765, Val Loss = 1.252811, LR = 0.000994\n",
      "Early stopping at epoch 25\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[59]\tvalid_0's l2: 0.00145999\n",
      "AIZ - Test MSE: 0.001374, R: 0.011248\n",
      "Training model for ADSK...\n",
      "Epoch 0: Train Loss = 1.778266, Val Loss = 0.630123, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[202]\tvalid_0's l2: 0.00120408\n",
      "ADSK - Test MSE: 0.001611, R: 0.040077\n",
      "Training model for ACGL...\n",
      "Epoch 0: Train Loss = 1.309562, Val Loss = 0.632771, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.00102285\n",
      "ACGL - Test MSE: 0.001234, R: -0.088132\n",
      "Training model for WFC...\n",
      "Epoch 0: Train Loss = 2.303341, Val Loss = 0.810562, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.00183963\n",
      "WFC - Test MSE: 0.002729, R: 0.015495\n",
      "Training model for AME...\n",
      "Epoch 0: Train Loss = 1.838145, Val Loss = 1.140067, LR = 0.000994\n",
      "Early stopping at epoch 22\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.000869841\n",
      "AME - Test MSE: 0.001175, R: 0.101348\n",
      "Training model for AOS...\n",
      "Epoch 0: Train Loss = 2.034614, Val Loss = 0.620289, LR = 0.000994\n",
      "Early stopping at epoch 21\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.000929948\n",
      "AOS - Test MSE: 0.001430, R: -0.181642\n",
      "Training model for AMD...\n",
      "Epoch 0: Train Loss = 2.371056, Val Loss = 0.383297, LR = 0.000994\n",
      "Early stopping at epoch 25\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.00275507\n",
      "AMD - Test MSE: 0.008661, R: -0.255030\n",
      "Training model for AMGN...\n",
      "Epoch 0: Train Loss = 2.054064, Val Loss = 0.976838, LR = 0.000994\n",
      "Epoch 50: Train Loss = 0.547485, Val Loss = 0.719366, LR = 0.000121\n",
      "Early stopping at epoch 95\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's l2: 0.00114003\n",
      "AMGN - Test MSE: 0.001398, R: -0.028407\n",
      "Training model for AIG...\n",
      "Epoch 0: Train Loss = 1.684084, Val Loss = 0.693892, LR = 0.000994\n",
      "Early stopping at epoch 22\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid_0's l2: 0.000473979\n",
      "AIG - Test MSE: 0.001114, R: -0.214189\n",
      "Training model for WMT...\n",
      "Epoch 0: Train Loss = 2.439194, Val Loss = 1.086536, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[50]\tvalid_0's l2: 0.000723283\n",
      "WMT - Test MSE: 0.002151, R: -0.059198\n",
      "Training model for AMZN...\n",
      "Epoch 0: Train Loss = 2.012003, Val Loss = 0.831672, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.00157857\n",
      "AMZN - Test MSE: 0.002608, R: 0.065841\n",
      "Training model for AON...\n",
      "Epoch 0: Train Loss = 2.222647, Val Loss = 0.835058, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's l2: 0.000748136\n",
      "AON - Test MSE: 0.001144, R: -0.076915\n",
      "Training model for AKAM...\n",
      "Epoch 0: Train Loss = 2.132108, Val Loss = 2.385134, LR = 0.000994\n",
      "Early stopping at epoch 21\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's l2: 0.00261995\n",
      "AKAM - Test MSE: 0.004570, R: 0.008144\n",
      "Training model for AFL...\n",
      "Epoch 0: Train Loss = 2.339078, Val Loss = 1.241293, LR = 0.000994\n",
      "Early stopping at epoch 22\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[81]\tvalid_0's l2: 0.000673996\n",
      "AFL - Test MSE: 0.000971, R: 0.012350\n",
      "Training model for TMO...\n",
      "Epoch 0: Train Loss = 2.478827, Val Loss = 1.207669, LR = 0.000994\n",
      "Early stopping at epoch 21\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's l2: 0.00126412\n",
      "TMO - Test MSE: 0.001787, R: -0.392516\n",
      "Training model for MS...\n",
      "Epoch 0: Train Loss = 1.927951, Val Loss = 0.879824, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's l2: 0.00191374\n",
      "MS - Test MSE: 0.003347, R: 0.006877\n",
      "Training model for AJG...\n",
      "Epoch 0: Train Loss = 1.798823, Val Loss = 1.177330, LR = 0.000994\n",
      "Early stopping at epoch 25\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.00127851\n",
      "AJG - Test MSE: 0.001396, R: 0.046601\n",
      "Training model for META...\n",
      "Epoch 0: Train Loss = 2.032636, Val Loss = 0.574938, LR = 0.000994\n",
      "Early stopping at epoch 22\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's l2: 0.001519\n",
      "META - Test MSE: 0.003940, R: -0.014838\n",
      "Training model for PG...\n",
      "Epoch 0: Train Loss = 2.137599, Val Loss = 1.216392, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[118]\tvalid_0's l2: 0.000571485\n",
      "PG - Test MSE: 0.000747, R: 0.017460\n",
      "Training model for V...\n",
      "Epoch 0: Train Loss = 2.272416, Val Loss = 1.072870, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.000624036\n",
      "V - Test MSE: 0.001385, R: 0.030459\n",
      "Training model for MA...\n",
      "Epoch 0: Train Loss = 1.633054, Val Loss = 0.589832, LR = 0.000994\n",
      "Early stopping at epoch 23\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.000532655\n",
      "MA - Test MSE: 0.001510, R: 0.029990\n",
      "Training model for AMT...\n",
      "Epoch 0: Train Loss = 1.566323, Val Loss = 0.689312, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[164]\tvalid_0's l2: 0.00149695\n",
      "AMT - Test MSE: 0.001370, R: -0.006758\n",
      "Training model for BAC...\n",
      "Epoch 0: Train Loss = 2.955999, Val Loss = 0.547045, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.000942706\n",
      "BAC - Test MSE: 0.002723, R: 0.066430\n",
      "Training model for ALGN...\n",
      "Epoch 0: Train Loss = 1.753890, Val Loss = 0.490100, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's l2: 0.00212095\n",
      "ALGN - Test MSE: 0.005713, R: -1.011604\n",
      "Training model for ATO...\n",
      "Epoch 0: Train Loss = 1.753192, Val Loss = 0.994757, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's l2: 0.000761156\n",
      "ATO - Test MSE: 0.000647, R: 0.023508\n",
      "Training model for CAT...\n",
      "Epoch 0: Train Loss = 2.016742, Val Loss = 1.107745, LR = 0.000994\n",
      "Early stopping at epoch 27\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's l2: 0.00222551\n",
      "CAT - Test MSE: 0.002328, R: -0.010755\n",
      "Training model for HD...\n",
      "Epoch 0: Train Loss = 1.506050, Val Loss = 0.457361, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.00098273\n",
      "HD - Test MSE: 0.001093, R: -0.045828\n",
      "Training model for ADI...\n",
      "Epoch 0: Train Loss = 1.557221, Val Loss = 0.878717, LR = 0.000994\n",
      "Epoch 50: Train Loss = 0.483797, Val Loss = 0.640663, LR = 0.000121\n",
      "Early stopping at epoch 56\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[129]\tvalid_0's l2: 0.00202262\n",
      "ADI - Test MSE: 0.004467, R: -0.008936\n",
      "Training model for ABBV...\n",
      "Epoch 0: Train Loss = 2.200782, Val Loss = 3.502189, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.00308396\n",
      "ABBV - Test MSE: 0.002209, R: -0.020820\n",
      "Training model for ABNB...\n",
      "Epoch 0: Train Loss = 1.675044, Val Loss = 0.925011, LR = 0.000994\n",
      "Early stopping at epoch 40\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.00287208\n",
      "ABNB - Test MSE: 0.003388, R: 0.120810\n",
      "Training model for APD...\n",
      "Epoch 0: Train Loss = 2.265697, Val Loss = 0.905798, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's l2: 0.00169708\n",
      "APD - Test MSE: 0.001319, R: -0.062932\n",
      "Training model for MRK...\n",
      "Epoch 0: Train Loss = 1.987374, Val Loss = 1.360424, LR = 0.000994\n",
      "Early stopping at epoch 26\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.00146506\n",
      "MRK - Test MSE: 0.002024, R: -0.045793\n",
      "Training model for MSFT...\n",
      "Epoch 0: Train Loss = 1.607238, Val Loss = 0.866635, LR = 0.000994\n",
      "Early stopping at epoch 23\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[67]\tvalid_0's l2: 0.000918276\n",
      "MSFT - Test MSE: 0.001719, R: -0.007846\n",
      "Training model for GE...\n",
      "Epoch 0: Train Loss = 1.563438, Val Loss = 0.766275, LR = 0.000994\n",
      "Early stopping at epoch 20\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.00150736\n",
      "GE - Test MSE: 0.003336, R: -0.026673\n",
      "Successfully trained models for 78 stocks\n",
      "=== PREDICTION GENERATION PHASE ===\n",
      "ANET: Prediction = -0.0045, Confidence = 0.8892\n",
      "GOOG: Prediction = 0.0057, Confidence = 0.9551\n",
      "AEE: Prediction = -0.0035, Confidence = 0.9803\n",
      "ACN: Prediction = 0.0017, Confidence = 0.9585\n",
      "AAPL: Prediction = 0.0030, Confidence = 0.9763\n",
      "JPM: Prediction = -0.0048, Confidence = 0.9860\n",
      "ADM: Prediction = 0.0213, Confidence = 0.9735\n",
      "ADBE: Prediction = 0.0372, Confidence = 0.9214\n",
      "AEP: Prediction = 0.0016, Confidence = 0.9851\n",
      "ALB: Prediction = 0.0017, Confidence = 0.8001\n",
      "APTV: Prediction = -0.0089, Confidence = 0.9662\n",
      "MCD: Prediction = -0.0009, Confidence = 0.9900\n",
      "APO: Prediction = -0.0153, Confidence = 0.8976\n",
      "ARE: Prediction = 0.0489, Confidence = 0.9416\n",
      "T: Prediction = 0.0056, Confidence = 0.9892\n",
      "TSLA: Prediction = -0.0489, Confidence = 0.8575\n",
      "AMP: Prediction = 0.0092, Confidence = 0.9883\n",
      "GS: Prediction = 0.0001, Confidence = 0.9802\n",
      "ALL: Prediction = 0.0090, Confidence = 0.9667\n",
      "AES: Prediction = 0.0664, Confidence = 0.9462\n",
      "ANSS: Prediction = -0.0033, Confidence = 0.9670\n",
      "JNJ: Prediction = -0.0022, Confidence = 0.9900\n",
      "UNH: Prediction = 0.0127, Confidence = 0.9804\n",
      "AMCR: Prediction = 0.0039, Confidence = 0.9862\n",
      "NVDA: Prediction = -0.0189, Confidence = 0.8963\n",
      "AWK: Prediction = -0.0036, Confidence = 0.9728\n",
      "A: Prediction = 0.0229, Confidence = 0.9726\n",
      "MMM: Prediction = 0.0072, Confidence = 0.8811\n",
      "GOOGL: Prediction = 0.0027, Confidence = 0.9605\n",
      "AMAT: Prediction = 0.0106, Confidence = 0.9688\n",
      "BA: Prediction = -0.0122, Confidence = 0.9504\n",
      "KO: Prediction = -0.0014, Confidence = 0.9900\n",
      "ABT: Prediction = -0.0133, Confidence = 0.9896\n",
      "CVX: Prediction = -0.0039, Confidence = 0.9860\n",
      "APA: Prediction = -0.0063, Confidence = 0.9515\n",
      "AXP: Prediction = 0.0092, Confidence = 0.9584\n",
      "MO: Prediction = 0.0095, Confidence = 0.9463\n",
      "PFE: Prediction = 0.0047, Confidence = 0.9674\n",
      "COST: Prediction = 0.0051, Confidence = 0.9899\n",
      "APH: Prediction = -0.0174, Confidence = 0.9695\n",
      "LNT: Prediction = 0.0056, Confidence = 0.9900\n",
      "NKE: Prediction = 0.0274, Confidence = 0.9399\n",
      "XOM: Prediction = -0.0019, Confidence = 0.9897\n",
      "ALLE: Prediction = 0.0016, Confidence = 0.9637\n",
      "AIZ: Prediction = 0.0024, Confidence = 0.9826\n",
      "ADSK: Prediction = -0.0004, Confidence = 0.9611\n",
      "ACGL: Prediction = 0.0049, Confidence = 0.9769\n",
      "WFC: Prediction = -0.0104, Confidence = 0.9814\n",
      "AME: Prediction = -0.0036, Confidence = 0.9865\n",
      "AOS: Prediction = 0.0107, Confidence = 0.9826\n",
      "AMD: Prediction = 0.0422, Confidence = 0.8381\n",
      "AMGN: Prediction = 0.0073, Confidence = 0.9621\n",
      "AIG: Prediction = -0.0127, Confidence = 0.9812\n",
      "WMT: Prediction = 0.0128, Confidence = 0.9814\n",
      "AMZN: Prediction = -0.0118, Confidence = 0.9786\n",
      "AON: Prediction = 0.0032, Confidence = 0.9708\n",
      "AKAM: Prediction = 0.0138, Confidence = 0.9869\n",
      "AFL: Prediction = 0.0106, Confidence = 0.9878\n",
      "TMO: Prediction = 0.0127, Confidence = 0.9737\n",
      "MS: Prediction = 0.0074, Confidence = 0.9800\n",
      "AJG: Prediction = 0.0028, Confidence = 0.9883\n",
      "META: Prediction = -0.0042, Confidence = 0.9713\n",
      "PG: Prediction = 0.0031, Confidence = 0.9900\n",
      "V: Prediction = -0.0050, Confidence = 0.9862\n",
      "MA: Prediction = 0.0075, Confidence = 0.9607\n",
      "AMT: Prediction = -0.0033, Confidence = 0.9556\n",
      "BAC: Prediction = -0.0083, Confidence = 0.9828\n",
      "ALGN: Prediction = 0.0692, Confidence = 0.9483\n",
      "ATO: Prediction = 0.0015, Confidence = 0.9866\n",
      "CAT: Prediction = 0.0017, Confidence = 0.9707\n",
      "HD: Prediction = 0.0115, Confidence = 0.9391\n",
      "ADI: Prediction = 0.0007, Confidence = 0.9430\n",
      "ABBV: Prediction = 0.0008, Confidence = 0.9605\n",
      "ABNB: Prediction = 0.0003, Confidence = 0.9122\n",
      "APD: Prediction = 0.0004, Confidence = 0.9543\n",
      "MRK: Prediction = 0.0067, Confidence = 0.9844\n",
      "MSFT: Prediction = -0.0016, Confidence = 0.9769\n",
      "GE: Prediction = 0.0035, Confidence = 0.9733\n",
      "Generated predictions for 78 stocks\n",
      "Prediction stats - Mean: 0.0044, Std: 0.0166, Min: -0.0489, Max: 0.0692\n",
      "Confidence stats - Mean: 0.9616, Min: 0.8001, Max: 0.9900\n",
      "=== RISK ANALYSIS PHASE ===\n",
      "Running Monte Carlo portfolio simulation...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk Analysis Results:\n",
      "  Expected Return: 0.0090\n",
      "  Volatility: 0.0049\n",
      "  95% VaR: 0.0009\n",
      "  Probability of Loss: 0.0320\n",
      "=== PORTFOLIO OPTIMIZATION PHASE ===\n",
      "Using 78 stocks with confidence >= 0.3\n",
      "Long positions: 50, Short positions: 28\n",
      "Total portfolio allocation: 0.9000 (target: 0.9)\n",
      "Portfolio Optimization Results:\n",
      "  Number of positions: 78\n",
      "  Total long weight: 0.6345\n",
      "  Total short weight: 0.2525\n",
      "  Net exposure: 0.3819\n",
      "=== TRADING EXECUTION PHASE ===\n",
      "Account value: $100,000.00\n",
      "No positions to close\n",
      "Liquidation Results:\n",
      "  Successful closes: 0\n",
      "  Failed closes: 0\n",
      "Executing 66 orders...\n",
      "Order submitted: BUY 3 shares of GOOG\n",
      "Order submitted: BUY 1 shares of AAPL\n",
      "Order submitted: BUY 46 shares of ADM\n",
      "Order submitted: BUY 10 shares of ADBE\n",
      "Order submitted: BUY 1 shares of AEP\n",
      "Order submitted: BUY 2 shares of ALB\n",
      "Order submitted: BUY 74 shares of ARE\n",
      "Order submitted: BUY 23 shares of T\n",
      "Order submitted: BUY 1 shares of AMP\n",
      "Order submitted: BUY 5 shares of ALL\n",
      "Order submitted: BUY 702 shares of AES\n",
      "Order submitted: BUY 4 shares of UNH\n",
      "Order submitted: BUY 50 shares of AMCR\n",
      "Order submitted: BUY 21 shares of A\n",
      "Order submitted: BUY 4 shares of MMM\n",
      "Order submitted: BUY 1 shares of GOOGL\n",
      "Order submitted: BUY 6 shares of AMAT\n",
      "Order submitted: BUY 3 shares of AXP\n",
      "Order submitted: BUY 18 shares of MO\n",
      "Order submitted: BUY 22 shares of PFE\n",
      "Order submitted: BUY 9 shares of LNT\n",
      "Order submitted: BUY 42 shares of NKE\n",
      "Order submitted: BUY 1 shares of ALLE\n",
      "Order submitted: BUY 1 shares of AIZ\n",
      "Order submitted: BUY 6 shares of ACGL\n",
      "Order submitted: BUY 19 shares of AOS\n",
      "Order submitted: BUY 27 shares of AMD\n",
      "Order submitted: BUY 2 shares of AMGN\n",
      "Order submitted: BUY 15 shares of WMT\n",
      "Order submitted: BUY 1 shares of AON\n",
      "Order submitted: BUY 20 shares of AKAM\n",
      "Order submitted: BUY 11 shares of AFL\n",
      "Order submitted: BUY 3 shares of TMO\n",
      "Order submitted: BUY 5 shares of MS\n",
      "Order submitted: BUY 1 shares of AJG\n",
      "Order submitted: BUY 2 shares of PG\n",
      "Order submitted: BUY 1 shares of MA\n",
      "Order submitted: BUY 40 shares of ALGN\n",
      "Order submitted: BUY 1 shares of ATO\n",
      "Order submitted: BUY 3 shares of HD\n",
      "Order submitted: BUY 9 shares of MRK\n",
      "Order submitted: BUY 1 shares of GE\n",
      "Order submitted: SELL 4 shares of ANET\n",
      "Order submitted: SELL 3 shares of AEE\n",
      "Order submitted: SELL 1 shares of JPM\n",
      "Order submitted: SELL 15 shares of APTV\n",
      "Order submitted: SELL 11 shares of APO\n",
      "Order submitted: SELL 15 shares of TSLA\n",
      "Order submitted: SELL 1 shares of ANSS\n",
      "Order submitted: SELL 1 shares of JNJ\n",
      "Order submitted: SELL 12 shares of NVDA\n",
      "Order submitted: SELL 3 shares of AWK\n",
      "Order submitted: SELL 6 shares of BA\n",
      "Order submitted: SELL 2 shares of KO\n",
      "Order submitted: SELL 11 shares of ABT\n",
      "Order submitted: SELL 3 shares of CVX\n",
      "Order submitted: SELL 38 shares of APA\n",
      "Order submitted: SELL 20 shares of APH\n",
      "Order submitted: SELL 1 shares of XOM\n",
      "Order submitted: SELL 13 shares of WFC\n",
      "Order submitted: SELL 2 shares of AME\n",
      "Order submitted: SELL 17 shares of AIG\n",
      "Order submitted: SELL 6 shares of AMZN\n",
      "Order submitted: SELL 1 shares of V\n",
      "Order submitted: SELL 1 shares of AMT\n",
      "Order submitted: SELL 17 shares of BAC\n",
      "Execution Results:\n",
      "  Successful orders: 66\n",
      "  Failed orders: 0\n",
      "=== PIPELINE COMPLETED SUCCESSFULLY ===\n",
      "Complete results saved to: ./results/complete_pipeline_results_20250701_042954.json\n",
      "Program completed\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Run complete pipeline\n",
    "        system = StockPredictionTradingSystem()\n",
    "        results = system.run_complete_pipeline(dry_run=False)\n",
    "        \n",
    "        # Save final results\n",
    "        results_file = os.path.join(config.RESULTS_SAVE_PATH, \n",
    "                                  f\"complete_pipeline_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"Complete results saved to: {results_file}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Execution interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"Execution failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    print(\"Program completed\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7765043,
     "sourceId": 12319125,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
